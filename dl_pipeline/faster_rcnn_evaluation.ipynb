{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b639e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for evaluation settings\n",
    "class EvaluationConfig:\n",
    "    def __init__(self):\n",
    "        # Paths and device configuration\n",
    "        self.DATA_ROOT = Path(\"./data\")\n",
    "        self.DATA_YAML = \"./data/data.yaml\"\n",
    "        self.CHECKPOINT_DIR = Path(\".\")\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.NUM_CLASSES = 13  # 12 classes + background\n",
    "        \n",
    "        # Evaluation settings\n",
    "        self.CONFIDENCE_THRESHOLDS = np.arange(0.0, 1.01, 0.005)\n",
    "        self.IOU_THRESHOLD = 0.3\n",
    "        self.RESULTS_DIR = Path(\"./results/faster_rcnn_evaluation\")\n",
    "\n",
    "config = EvaluationConfig()\n",
    "class_names = ['background', 'Ants', 'Bees', 'Beetles', 'Caterpillars', 'Earthworms', 'Earwigs', 'Grasshoppers', 'Moths', 'Slugs', 'Snails', 'Wasps', 'Weevils']\n",
    "\n",
    "print(f\"Current directory: {Path.cwd()}\")\n",
    "print(f\"Data root: {config.DATA_ROOT.resolve()}\")\n",
    "print(f\"Results will be saved to: {config.RESULTS_DIR.resolve()}\")\n",
    "print(f\"Checkpoint dir: {config.CHECKPOINT_DIR.resolve()}\")\n",
    "print(f\"Classes ({len(class_names)-1}): {class_names[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d26cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the trained Faster R-CNN model\n",
    "def load_trained_model():\n",
    "    # Create model architecture\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, config.NUM_CLASSES)\n",
    "    \n",
    "    # Load trained weights by checking multiple possible locations\n",
    "    checkpoint_paths = [\n",
    "        config.CHECKPOINT_DIR / 'faster_rcnn_checkpoint_epoch_12.pth',\n",
    "        config.CHECKPOINT_DIR / 'best_faster_rcnn_model.pth',\n",
    "        config.CHECKPOINT_DIR / 'faster_rcnn_checkpoint_epoch_10.pth',\n",
    "        config.CHECKPOINT_DIR / 'faster_rcnn_checkpoint_epoch_8.pth',\n",
    "    ]\n",
    "    \n",
    "    # Find the first existing checkpoint\n",
    "    checkpoint_path = None\n",
    "    for path in checkpoint_paths:\n",
    "        if path.exists():\n",
    "            checkpoint_path = path\n",
    "            print(f\"Found checkpoint: {path}\")\n",
    "            break\n",
    "    \n",
    "    # If no checkpoint found, raise an error listing available .pth files\n",
    "    if checkpoint_path is None:\n",
    "        available = list(config.CHECKPOINT_DIR.glob('*.pth'))\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find checkpoint. Available .pth files:\\n\" +\n",
    "            \"\\n\".join([f\"  - {p}\" for p in available])\n",
    "        )\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(config.DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from: {checkpoint_path}\")\n",
    "    print(f\"Epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "    print(f\"Training loss: {checkpoint.get('train_loss', 'Unknown'):.4f}\")\n",
    "    print(f\"Validation loss: {checkpoint.get('valid_loss', 'Unknown'):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "trained_model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load ground truth annotations from test set\n",
    "def load_ground_truth_annotations():\n",
    "    # Directories for test labels and images\n",
    "    test_labels_dir = config.DATA_ROOT / 'test' / 'labels'\n",
    "    test_images_dir = config.DATA_ROOT / 'test' / 'images'\n",
    "    \n",
    "    label_files = list(test_labels_dir.glob('*.txt'))\n",
    "    \n",
    "    annotations = {}\n",
    "    images_not_found = 0\n",
    "    \n",
    "    # Process each label file\n",
    "    for i, label_file in enumerate(label_files):\n",
    "        img_name = label_file.stem\n",
    "        \n",
    "        # Find corresponding image\n",
    "        img_path = None\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            potential_path = test_images_dir / f\"{img_name}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        # If no image found, log and continue\n",
    "        if img_path is None:\n",
    "            images_not_found += 1\n",
    "            if i < 3:\n",
    "                print(f\"   No matching image found!\")\n",
    "            continue\n",
    "            \n",
    "        # Load image dimensions\n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Parse YOLO format annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # Read label file\n",
    "        try:\n",
    "            with open(label_file, 'r') as f:\n",
    "                lines = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "            # Convert YOLO format to bounding boxes\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0]) + 1  # Convert to 1-indexed\n",
    "                    center_x = float(parts[1]) * img_width\n",
    "                    center_y = float(parts[2]) * img_height\n",
    "                    width = float(parts[3]) * img_width\n",
    "                    height = float(parts[4]) * img_height\n",
    "                    \n",
    "                    x1 = center_x - width / 2\n",
    "                    y1 = center_y - height / 2\n",
    "                    x2 = center_x + width / 2\n",
    "                    y2 = center_y + height / 2\n",
    "                    \n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(class_id)\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Store annotations if any boxes found\n",
    "        if len(boxes) > 0:\n",
    "            annotations[str(img_path)] = {\n",
    "                'boxes': np.array(boxes),\n",
    "                'labels': np.array(labels)\n",
    "            }\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   Total label files: {len(label_files)}\")\n",
    "    print(f\"   Images not found: {images_not_found}\")\n",
    "    print(f\"   Loaded {len(annotations)} test images with annotations\")\n",
    "    return annotations\n",
    "\n",
    "# Load ground truth\n",
    "gt_annotations = load_ground_truth_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ae771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the model on test set and collect predictions\n",
    "def evaluate_model():    \n",
    "    # Image transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((640, 640)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process each test image\n",
    "    for i, (img_path, gt_data) in enumerate(gt_annotations.items()):\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            original_size = image.size\n",
    "            input_tensor = transform(image).unsqueeze(0).to(config.DEVICE)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            \n",
    "            # Process predictions\n",
    "            pred = predictions[0]\n",
    "            pred_boxes = pred['boxes'].cpu().numpy()\n",
    "            pred_scores = pred['scores'].cpu().numpy()\n",
    "            pred_labels = pred['labels'].cpu().numpy()\n",
    "            \n",
    "            # Scale boxes back to original size\n",
    "            if len(pred_boxes) > 0:\n",
    "                scale_x = original_size[0] / 640\n",
    "                scale_y = original_size[1] / 640\n",
    "                pred_boxes[:, [0, 2]] *= scale_x\n",
    "                pred_boxes[:, [1, 3]] *= scale_y\n",
    "            \n",
    "            # Store predictions\n",
    "            for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "                all_predictions.append({\n",
    "                    'image': img_path,\n",
    "                    'box': box,\n",
    "                    'score': score,\n",
    "                    'label': label\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Evaluation complete: {len(all_predictions)} total predictions\")\n",
    "    return all_predictions\n",
    "\n",
    "# Run evaluation\n",
    "predictions = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ac9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IoU between two boxes\n",
    "# Args:\n",
    "#   - box1, box2: Lists or arrays of [x1, y1, x2, y2]\n",
    "# Returns:\n",
    "#   - IoU value (float)\n",
    "def calculate_iou(box1, box2):\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate intersection and union areas\n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Function to calculate precision, recall, F1 at a given confidence threshold\n",
    "# Args:\n",
    "#   - conf_threshold: Confidence threshold to filter predictions\n",
    "# Returns:\n",
    "#   - metrics: Dictionary of precision, recall, F1 for each class\n",
    "def calculate_metrics_at_confidence(conf_threshold=0.5):    \n",
    "    metrics = {}\n",
    "    total_predictions_made = 0\n",
    "    total_matches_found = 0\n",
    "    \n",
    "    # Process each class except background\n",
    "    for class_idx in range(1, len(class_names)):\n",
    "        class_name = class_names[class_idx]\n",
    "        \n",
    "        # Filter predictions by class and confidence\n",
    "        class_preds = [p for p in predictions \n",
    "                      if p['label'] == class_idx and p['score'] >= conf_threshold]\n",
    "        \n",
    "        # Get ground truth for this class\n",
    "        class_gt = []\n",
    "        for img_path, gt_data in gt_annotations.items():\n",
    "            for box, label in zip(gt_data['boxes'], gt_data['labels']):\n",
    "                if label == class_idx:\n",
    "                    class_gt.append({'image': img_path, 'box': box, 'label': label})\n",
    "        \n",
    "        total_predictions_made += len(class_preds)\n",
    "        \n",
    "        # Calculate matches\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        \n",
    "        image_groups = defaultdict(lambda: {'pred': [], 'gt': []})\n",
    "        \n",
    "        for pred in class_preds:\n",
    "            image_groups[pred['image']]['pred'].append(pred)\n",
    "        \n",
    "        for gt in class_gt:\n",
    "            image_groups[gt['image']]['gt'].append(gt)\n",
    "        \n",
    "        matches_for_class = 0\n",
    "        \n",
    "        # Process each image\n",
    "        for img_path, data in image_groups.items():\n",
    "            pred_boxes = [p['box'] for p in data['pred']]\n",
    "            gt_boxes = [g['box'] for g in data['gt']]\n",
    "            \n",
    "            matched_gt = set()\n",
    "            \n",
    "            # Match predictions to ground truth boxes based on IoU\n",
    "            for pred_box in pred_boxes:\n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                # Find best matching ground truth box\n",
    "                for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                    if gt_idx in matched_gt:\n",
    "                        continue\n",
    "                    \n",
    "                    iou = calculate_iou(pred_box, gt_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                # Determine if it's a TP or FP\n",
    "                if best_iou >= config.IOU_THRESHOLD:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(best_gt_idx)\n",
    "                    matches_for_class += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            \n",
    "            fn += len(gt_boxes) - len(matched_gt)\n",
    "        \n",
    "        total_matches_found += matches_for_class\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        metrics[class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics_05 = calculate_metrics_at_confidence(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "config.RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to plot Precision-Recall curves for each class\n",
    "def plot_precision_recall_curves():    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for class_idx in range(1, len(class_names)):\n",
    "        class_name = class_names[class_idx]\n",
    "        \n",
    "        # Collect data for PR curve\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        \n",
    "        # Get all predictions and ground truth for this class\n",
    "        class_preds = [p for p in predictions if p['label'] == class_idx]\n",
    "        \n",
    "        for img_path, gt_data in gt_annotations.items():\n",
    "            img_gt = [g for g in gt_data['labels'] if g == class_idx]\n",
    "            img_preds = [p for p in class_preds if p['image'] == img_path]\n",
    "            \n",
    "            # Sort predictions by score\n",
    "            img_preds.sort(key=lambda x: x['score'], reverse=True)\n",
    "            \n",
    "            # Calculate matches\n",
    "            for pred in img_preds:\n",
    "                # Check if this prediction matches any ground truth\n",
    "                is_match = len(img_gt) > 0\n",
    "                y_true.append(1 if is_match else 0)\n",
    "                y_scores.append(pred['score'])\n",
    "        \n",
    "        if len(y_true) > 0:\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "            ap = average_precision_score(y_true, y_scores) if len(set(y_true)) > 1 else 0\n",
    "            \n",
    "            plt.plot(recall, precision, label=f'{class_name} {ap:.3f}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'BoxPR_curve.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"PR curve saved: {save_path}\")\n",
    "\n",
    "# Generate plot\n",
    "plot_precision_recall_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot F1-confidence curves\n",
    "def plot_f1_confidence_curves():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    all_f1_scores = []\n",
    "    confidence_range = np.arange(0.0, 1.01, 0.005) \n",
    "    \n",
    "    # Calculate F1 for each confidence threshold\n",
    "    for conf_threshold in confidence_range:\n",
    "        metrics = calculate_metrics_at_confidence(conf_threshold)\n",
    "        avg_f1 = np.mean([m['f1'] for m in metrics.values()]) if metrics else 0\n",
    "        all_f1_scores.append(avg_f1)\n",
    "    \n",
    "    # Plot average F1\n",
    "    plt.plot(config.CONFIDENCE_THRESHOLDS, all_f1_scores, \n",
    "             linewidth=3, label=f'all classes {max(all_f1_scores):.2f}')\n",
    "    \n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('F1')\n",
    "    plt.title('F1-Confidence Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'BoxF1_curve.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"F1 curve saved: {save_path}\")\n",
    "\n",
    "# Generate plot\n",
    "plot_f1_confidence_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate confusion matrix\n",
    "def generate_confusion_matrix():\n",
    "    # Collect all predictions and ground truth for confusion matrix\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    conf_threshold = 0.05\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path, gt_data in gt_annotations.items():\n",
    "        # Get predictions for this image\n",
    "        img_preds = [p for p in predictions if p['image'] == img_path and p['score'] >= conf_threshold]\n",
    "        \n",
    "        # Get ground truth for this image\n",
    "        img_gt_boxes = gt_data['boxes']\n",
    "        img_gt_labels = gt_data['labels']\n",
    "        \n",
    "        # Match predictions to ground truth using IoU\n",
    "        matched_gt = set()\n",
    "        \n",
    "        for pred in img_preds:\n",
    "            pred_box = pred['box']\n",
    "            pred_label = pred['label']\n",
    "            \n",
    "            best_iou = 0\n",
    "            best_gt_label = None\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            # Find best matching ground truth\n",
    "            for gt_idx, (gt_box, gt_label) in enumerate(zip(img_gt_boxes, img_gt_labels)):\n",
    "                if gt_idx in matched_gt:\n",
    "                    continue\n",
    "                \n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_label = gt_label\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            if best_iou >= config.IOU_THRESHOLD:\n",
    "                # True positive/correct prediction\n",
    "                y_true.append(best_gt_label)\n",
    "                y_pred.append(pred_label)\n",
    "                matched_gt.add(best_gt_idx)\n",
    "            else:\n",
    "                # False positive/wrong prediction\n",
    "                y_true.append(0) # Background\n",
    "                y_pred.append(pred_label)\n",
    "        \n",
    "        # Add false negatives (missed ground truth)\n",
    "        for gt_idx, gt_label in enumerate(img_gt_labels):\n",
    "            if gt_idx not in matched_gt:\n",
    "                y_true.append(gt_label)\n",
    "                y_pred.append(0)  # Predicted as background\n",
    "    \n",
    "    if len(y_true) == 0:\n",
    "        print(\"WARNING: No data for confusion matrix!\")\n",
    "        return\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    labels = list(range(len(class_names)))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'confusion_matrix.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'confusion_matrix_normalized.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Confusion matrices saved\")\n",
    "    return cm\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix_result = generate_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f17c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Recall-Confidence curves\n",
    "def plot_recall_confidence_curves():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    all_recall_scores = []\n",
    "    confidence_range = np.arange(0.0, 1.01, 0.005) \n",
    "    \n",
    "    # Calculate Recall for each confidence threshold\n",
    "    for conf_threshold in confidence_range:\n",
    "        metrics = calculate_metrics_at_confidence(conf_threshold)\n",
    "        avg_recall = np.mean([m['recall'] for m in metrics.values()]) if metrics else 0\n",
    "        all_recall_scores.append(avg_recall)\n",
    "    \n",
    "    # Plot average recall\n",
    "    plt.plot(config.CONFIDENCE_THRESHOLDS, all_recall_scores, \n",
    "             color='blue', linewidth=3, \n",
    "             label=f'all classes {max(all_recall_scores):.2f} at {config.CONFIDENCE_THRESHOLDS[np.argmax(all_recall_scores)]:.3f}')\n",
    "    \n",
    "    # Plot individual classes\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)-1))\n",
    "    \n",
    "    # Plot recall for each class\n",
    "    for class_idx in range(1, len(class_names)):\n",
    "        class_name = class_names[class_idx]\n",
    "        class_recall_scores = []\n",
    "        \n",
    "        for conf_threshold in confidence_range:\n",
    "            metrics = calculate_metrics_at_confidence(conf_threshold)\n",
    "            recall = metrics.get(class_name, {}).get('recall', 0)\n",
    "            class_recall_scores.append(recall)\n",
    "        \n",
    "        plt.plot(config.CONFIDENCE_THRESHOLDS, class_recall_scores,\n",
    "                color=colors[class_idx-1], label=class_name, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Confidence', fontsize=12)\n",
    "    plt.ylabel('Recall', fontsize=12)\n",
    "    plt.title('Recall-Confidence Curve', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'BoxR_curve.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Recall curve saved: {save_path}\")\n",
    "\n",
    "# Generate plot\n",
    "plot_recall_confidence_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Precision-Confidence curves\n",
    "def plot_precision_confidence_curves():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    all_precision_scores = []\n",
    "    confidence_range = np.arange(0.0, 1.01, 0.005) \n",
    "    \n",
    "    # Calculate Precision for each confidence threshold\n",
    "    for conf_threshold in confidence_range:\n",
    "        metrics = calculate_metrics_at_confidence(conf_threshold)\n",
    "        avg_precision = np.mean([m['precision'] for m in metrics.values()]) if metrics else 0\n",
    "        all_precision_scores.append(avg_precision)\n",
    "    \n",
    "    # Plot average precision\n",
    "    plt.plot(config.CONFIDENCE_THRESHOLDS, all_precision_scores, \n",
    "             color='blue', linewidth=3, \n",
    "             label=f'all classes {max(all_precision_scores):.2f} at {config.CONFIDENCE_THRESHOLDS[np.argmax(all_precision_scores)]:.3f}')\n",
    "    \n",
    "    # Plot individual classes\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)-1))\n",
    "    \n",
    "    # Plot precision for each class\n",
    "    for class_idx in range(1, len(class_names)):\n",
    "        class_name = class_names[class_idx]\n",
    "        class_precision_scores = []\n",
    "        \n",
    "        for conf_threshold in confidence_range:\n",
    "            metrics = calculate_metrics_at_confidence(conf_threshold)\n",
    "            precision = metrics.get(class_name, {}).get('precision', 0)\n",
    "            class_precision_scores.append(precision)\n",
    "        \n",
    "        plt.plot(config.CONFIDENCE_THRESHOLDS, class_precision_scores,\n",
    "                color=colors[class_idx-1], label=class_name, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Confidence', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Confidence Curve', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    save_path = config.RESULTS_DIR / 'BoxP_curve.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Precision curve saved: {save_path}\")\n",
    "\n",
    "# Generate plot\n",
    "plot_precision_confidence_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create complete evaluation summary matching YOLO output\n",
    "def create_comprehensive_summary():    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision = np.mean([m['precision'] for m in metrics_05.values()])\n",
    "    overall_recall = np.mean([m['recall'] for m in metrics_05.values()])\n",
    "    overall_f1 = np.mean([m['f1'] for m in metrics_05.values()])\n",
    "\n",
    "    # Save complete summary\n",
    "    summary = {\n",
    "        'model': 'Faster R-CNN',\n",
    "        'dataset': 'AgroPest-12',\n",
    "        'overall_metrics': {\n",
    "            'precision': float(overall_precision),\n",
    "            'recall': float(overall_recall),\n",
    "            'f1_score': float(overall_f1)\n",
    "        },\n",
    "        'class_metrics': {k: {key: float(val) for key, val in v.items()} \n",
    "                        for k, v in metrics_05.items()},\n",
    "        'evaluation_settings': {\n",
    "            'iou_threshold': config.IOU_THRESHOLD,\n",
    "            'confidence_threshold': 0.05,\n",
    "            'num_test_images': len(gt_annotations)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary as evaluation_summary.json\n",
    "    summary_path = config.RESULTS_DIR / 'evaluation_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nAll results saved to: {config.RESULTS_DIR}/\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "final_results = create_comprehensive_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
