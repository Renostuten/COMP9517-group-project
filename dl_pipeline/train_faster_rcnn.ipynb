{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee04dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Faster R-CNN Training Setup ===\n",
      "Python version: 3.10.18\n",
      "PyTorch version: 2.7.1+cu118\n",
      "Torchvision version: 0.22.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "CUDA memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import yaml\n",
    "\n",
    "print(\"=== Faster R-CNN Training Setup ===\")\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data configuration from yaml_path\n",
    "# Returns a dict with paths, number of classes, and class names\n",
    "def load_data_config(yaml_path):\n",
    "    yaml_path = Path(yaml_path)\n",
    "    \n",
    "    if not yaml_path.exists():\n",
    "        raise FileNotFoundError(f\"data.yaml not found at {yaml_path}\")\n",
    "    \n",
    "    with open(yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    print(f\"Loaded data configuration from {yaml_path}\")\n",
    "    print(f\"Number of classes: {config['nc']}\")\n",
    "    print(f\"Class names: {config['names']}\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7396ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data configuration from ..\\data\\data.yaml\n",
      "Number of classes: 12\n",
      "Class names: ['Ants', 'Bees', 'Beetles', 'Caterpillars', 'Earthworms', 'Earwigs', 'Grasshoppers', 'Moths', 'Slugs', 'Snails', 'Wasps', 'Weevils']\n",
      "Training on device: cuda\n",
      "Number of classes: 13\n"
     ]
    }
   ],
   "source": [
    "# Configuration class to store all hyperparameters and settings\n",
    "class Config:\n",
    "    DATA_ROOT = Path(\"../data\")\n",
    "    DATA_YAML = \"../data/data.yaml\"\n",
    "    \n",
    "    USE_SUBSET = None\n",
    "    SUBSET_VALID = None\n",
    "    \n",
    "    # Load dataset configuration from YAML file\n",
    "    try:\n",
    "        data_config = load_data_config(DATA_YAML)\n",
    "        NUM_CLASSES = data_config['nc'] + 1  # plus 1 for background class\n",
    "        CLASS_NAMES = ['background'] + data_config['names']  # Add background as class 0\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: data.yaml not found\")\n",
    "    \n",
    "    # Model configuration\n",
    "    BACKBONE = 'resnet50'\n",
    "    PRETRAINED = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 0.002\n",
    "    NUM_EPOCHS = 6\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    MOMENTUM = 0.9\n",
    "    \n",
    "    # Image processing\n",
    "    IMG_SIZE = (416, 416)  # Input image size (height, width)\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Training settings\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_WORKERS = 0\n",
    "    PRINT_FREQ = 50\n",
    "    SAVE_FREQ = 2\n",
    "    \n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    # Learning rate scheduler settings\n",
    "    LR_STEP_SIZE = 2\n",
    "    LR_GAMMA = 0.3\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "print(f\"Training on device: {config.DEVICE}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc6e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DATASET CLASS FOR LOADING AGROPEST DATA\n",
    "class AgroPestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for loading AgroPest-12 data\n",
    "    \n",
    "    This class handles:\n",
    "    - Loading images from the dataset\n",
    "    - Converting YOLO format annotations to Faster R-CNN format\n",
    "    - Applying data augmentations\n",
    "    - Returning properly formatted tensors for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str/Path): Root directory containing train/valid/test folders\n",
    "            split (str): Which split to use ('train', 'valid', or 'test')\n",
    "            transform (callable): Optional transform to apply to images\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Set paths for images and labels\n",
    "        self.image_dir = self.data_dir / split / 'images'\n",
    "        self.label_dir = self.data_dir / split / 'labels'\n",
    "        \n",
    "        # Get list of all image files\n",
    "        self.image_paths = list(self.image_dir.glob('*.jpg'))\n",
    "        \n",
    "        # Use subset for faster training/testing if specified\n",
    "        if hasattr(config, 'USE_SUBSET') and config.USE_SUBSET is not None:\n",
    "            if split == 'train' and config.USE_SUBSET:\n",
    "                self.image_paths = self.image_paths[:config.USE_SUBSET]\n",
    "                print(f\"Using subset: {len(self.image_paths)} images (from {config.USE_SUBSET} requested)\")\n",
    "            elif split == 'valid' and hasattr(config, 'SUBSET_VALID') and config.SUBSET_VALID:\n",
    "                self.image_paths = self.image_paths[:config.SUBSET_VALID]\n",
    "                print(f\"Using validation subset: {len(self.image_paths)} images\")\n",
    "                \n",
    "        # Verify that image and label directories exist\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n",
    "        if not self.label_dir.exists():\n",
    "            raise FileNotFoundError(f\"Label directory not found: {self.label_dir}\")\n",
    "            \n",
    "        print(f\"Loaded {len(self.image_paths)} images from {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, target) where image is a tensor and target is a dict\n",
    "        \"\"\"\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load annotations\n",
    "        label_path = self.label_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        boxes, labels = self._load_yolo_annotations(label_path, image.size)\n",
    "        \n",
    "        # Prepare target dictionary (format required by Faster R-CNN)\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': self._calculate_area(boxes),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def _load_yolo_annotations(self, label_path, img_size):\n",
    "        \"\"\"\n",
    "        Load and convert YOLO format annotations to Faster R-CNN format\n",
    "        \n",
    "        YOLO format: class_id center_x center_y width height (normalized 0-1)\n",
    "        Faster R-CNN format: [x_min, y_min, x_max, y_max] (absolute pixels)\n",
    "        \n",
    "        Args:\n",
    "            label_path (Path): Path to the YOLO label file\n",
    "            img_size (tuple): (width, height) of the image\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (boxes, labels) as lists\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        img_width, img_height = img_size\n",
    "        \n",
    "        # Check if label file exists (some images might not have annotations)\n",
    "        if not label_path.exists():\n",
    "            # Return empty annotations for images without labels\n",
    "            return [], []\n",
    "        \n",
    "        # Read YOLO format annotations\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "                \n",
    "            # Parse YOLO format: class_id center_x center_y width height\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip malformed lines\n",
    "                \n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1])\n",
    "            center_y = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            # Convert normalized coordinates to absolute coordinates\n",
    "            center_x_abs = center_x * img_width\n",
    "            center_y_abs = center_y * img_height\n",
    "            width_abs = width * img_width\n",
    "            height_abs = height * img_height\n",
    "            \n",
    "            # Convert center format to corner format\n",
    "            x_min = center_x_abs - width_abs / 2\n",
    "            y_min = center_y_abs - height_abs / 2\n",
    "            x_max = center_x_abs + width_abs / 2\n",
    "            y_max = center_y_abs + height_abs / 2\n",
    "            \n",
    "            # Ensure coordinates are within image bounds\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(img_width, x_max)\n",
    "            y_max = min(img_height, y_max)\n",
    "            \n",
    "            # Only add valid boxes (with positive area)\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(class_id + 1)  # Add 1 because Faster R-CNN uses 1-indexed labels\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def _calculate_area(self, boxes):\n",
    "        \"\"\"Calculate area of bounding boxes\"\"\"\n",
    "        if len(boxes) == 0:\n",
    "            return torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        return (boxes_tensor[:, 2] - boxes_tensor[:, 0]) * (boxes_tensor[:, 3] - boxes_tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfdc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. DATA TRANSFORMS AND AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_transforms(split='train'):\n",
    "    \"\"\"\n",
    "    Get data transforms for training or validation\n",
    "    \n",
    "    Args:\n",
    "        split (str): 'train' for training transforms, anything else for validation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Composed transforms\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        # Training transforms with data augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color augmentation\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])\n",
    "    else:\n",
    "        # Validation transforms without augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05328bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. MODEL CREATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def create_faster_rcnn_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create a Faster R-CNN model with custom number of classes\n",
    "    \n",
    "    How Faster R-CNN works:\n",
    "    1. Backbone (ResNet-50): Extracts features from input image\n",
    "    2. FPN (Feature Pyramid Network): Combines features at different scales\n",
    "    3. RPN (Region Proposal Network): Generates potential object locations\n",
    "    4. ROI Head: Classifies proposals and refines bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of classes including background\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Faster R-CNN model\n",
    "    \"\"\"\n",
    "    # Load pre-trained Faster R-CNN model with ResNet-50 backbone and FPN\n",
    "    # This model is pre-trained on COCO dataset (80 classes + background)\n",
    "    from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "    if config.PRETRAINED:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    # The classifier head takes features from the ROI pooling layer\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one for our number of classes\n",
    "    # FastRCNNPredictor includes both classification and bounding box regression\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    print(f\"Created Faster R-CNN model with {num_classes} classes\")\n",
    "    print(f\"Backbone: ResNet-50 + FPN\")\n",
    "    print(f\"Classifier input features: {in_features}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6642a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. COLLATE FUNCTION FOR DATALOADER\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per image\n",
    "    \n",
    "    Faster R-CNN needs a special collate function because:\n",
    "    - Different images have different numbers of objects\n",
    "    - We can't stack tensors with different shapes\n",
    "    - We need to return lists of images and targets\n",
    "    \n",
    "    Args:\n",
    "        batch (list): List of (image, target) tuples\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (images, targets) as lists\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=5):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize mixed precision scaler\n",
    "    if config.MIXED_PRECISION:\n",
    "        try:\n",
    "            scaler = GradScaler(\"cuda\")\n",
    "        except TypeError:\n",
    "            scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Move images and targets to device (GPU)\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass with mixed precision if enabled\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            try:\n",
    "                with autocast(\"cuda\"):  # New API\n",
    "                    loss_dict = model(images, targets)\n",
    "            except TypeError:\n",
    "                with autocast():        # Fallback\n",
    "                    loss_dict = model(images, targets)\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        total_loss = total_loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            scaler.scale(total_loss).backward()  # Scaled backward pass\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        losses.append(total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS)  # Store unscaled loss\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "        if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if config.MIXED_PRECISION and scaler is not None:\n",
    "                scaler.step(optimizer)  # Update weights with scaling\n",
    "                scaler.update()  # Update scaler for next iteration\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "        if batch_idx % print_freq == 0 or batch_idx == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "            print(f'Epoch [{epoch}], Batch [{batch_idx}/{len(data_loader)}], '\n",
    "                  f'Loss: {total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS:.4f}, '\n",
    "                  f'LR: {current_lr:.6f}, GPU Memory: {memory_used:.2f}GB')\n",
    "            \n",
    "            # Print individual loss components for debugging (less frequently)\n",
    "            if batch_idx % (print_freq * 2) == 0:\n",
    "                for loss_name, loss_value in loss_dict.items():\n",
    "                    print(f'  {loss_name}: {loss_value.item():.4f}')\n",
    "        \n",
    "        # Clear cache periodically to prevent memory fragmentation\n",
    "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def validate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model (calculate validation loss)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader with validation data\n",
    "        device: Device to run validation on\n",
    "        \n",
    "    Returns:\n",
    "        float: Average validation loss\n",
    "    \"\"\"\n",
    "    model.train()  # Keep in training mode to get loss values\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for images, targets in data_loader:\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.append(total_loss.item())\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6aa6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 9. MAIN TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_faster_rcnn():\n",
    "    \"\"\"\n",
    "    Main function to train the Faster R-CNN model\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT, \n",
    "        split='train',\n",
    "        transform=get_transforms('train')\n",
    "    )\n",
    "    \n",
    "    valid_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT,\n",
    "        split='valid', \n",
    "        transform=get_transforms('valid')\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,  # Shuffle training data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn  # Use custom collate function\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,  # Don't shuffle validation data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Valid batches: {len(valid_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_faster_rcnn_model(config.NUM_CLASSES)\n",
    "    model.to(config.DEVICE)\n",
    "    \n",
    "    # Create optimizer\n",
    "    # Using SGD with momentum (common choice for object detection)\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        momentum=config.MOMENTUM,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (reduce LR when training stagnates)\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.LR_STEP_SIZE,  # Use config value\n",
    "        gamma=config.LR_GAMMA          # Use config value\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {config.NUM_EPOCHS} epochs...\")\n",
    "    print(f\"  - Batch size: {config.BATCH_SIZE} (with gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS})\")\n",
    "    print(f\"  - Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - Image size: {config.IMG_SIZE}\")\n",
    "    print(f\"  - Mixed precision: {config.MIXED_PRECISION}\")\n",
    "    print(f\"  - Memory monitoring enabled\")\n",
    "    \n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  - Initial GPU memory: {initial_memory:.2f}GB / 4.3GB\")\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{config.NUM_EPOCHS} ===\")\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, config.DEVICE, epoch + 1, config.PRINT_FREQ\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        valid_loss = validate_model(model, valid_loader, config.DEVICE)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Memory monitoring for RTX 3050 Ti\n",
    "        if torch.cuda.is_available():\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "            torch.cuda.reset_peak_memory_stats()  # Reset for next epoch\n",
    "        else:\n",
    "            peak_memory = current_memory = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.1f}s, Peak GPU Memory: {peak_memory:.2f}GB\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Memory warning\n",
    "        if peak_memory > 4.0:\n",
    "            print(f\"    WARNING: High memory usage ({peak_memory:.2f}GB). Consider reducing batch size or image size.\")\n",
    "        \n",
    "        # Save best model\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__  # Save all config settings\n",
    "            }, 'best_faster_rcnn_model.pth')\n",
    "            print(f\"  ✅ New best model saved! Validation loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every few epochs\n",
    "        if (epoch + 1) % config.SAVE_FREQ == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__\n",
    "            }, f'faster_rcnn_checkpoint_epoch_{epoch + 1}.pth')\n",
    "    \n",
    "    # Plot training curves    \n",
    "    plt.close('all')\n",
    "    \n",
    "    # Create plot with error handling\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(train_losses, label='Training Loss', marker='o')\n",
    "    ax.plot(valid_losses, label='Validation Loss', marker='s')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Faster R-CNN Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png', dpi=100, bbox_inches='tight')\n",
    "    print(f\"✅ Training curves saved in '../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png'\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275af32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Faster R-CNN training for AgroPest-12 dataset\n",
      "Configuration:\n",
      "  Classes: 13\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.002\n",
      "  Epochs: 6\n",
      "  Device: cuda\n",
      "Loading datasets...\n",
      "Loaded 11502 images from train split\n",
      "Loaded 1095 images from valid split\n",
      "Train batches: 1438\n",
      "Valid batches: 137\n",
      "Created Faster R-CNN model with 13 classes\n",
      "Backbone: ResNet-50 + FPN\n",
      "Classifier input features: 1024\n",
      "Starting training for 6 epochs...\n",
      "  - Batch size: 8 (with gradient accumulation: 2)\n",
      "  - Effective batch size: 16\n",
      "  - Image size: (416, 416)\n",
      "  - Mixed precision: True\n",
      "  - Memory monitoring enabled\n",
      "  - Initial GPU memory: 0.17GB / 4.3GB\n",
      "\n",
      "=== Epoch 1/6 ===\n",
      "Epoch [1], Batch [0/1438], Loss: 3.3427, LR: 0.002000, GPU Memory: 0.38GB\n",
      "  loss_classifier: 2.8737\n",
      "  loss_box_reg: 0.0336\n",
      "  loss_objectness: 0.3110\n",
      "  loss_rpn_box_reg: 0.1244\n",
      "Epoch [1], Batch [50/1438], Loss: 0.1166, LR: 0.002000, GPU Memory: 0.54GB\n",
      "Epoch [1], Batch [100/1438], Loss: 0.1803, LR: 0.002000, GPU Memory: 0.54GB\n",
      "  loss_classifier: 0.0795\n",
      "  loss_box_reg: 0.0435\n",
      "  loss_objectness: 0.0188\n",
      "  loss_rpn_box_reg: 0.0386\n",
      "Epoch [1], Batch [150/1438], Loss: 0.0671, LR: 0.002000, GPU Memory: 0.54GB\n",
      "Epoch [1], Batch [200/1438], Loss: 1.4626, LR: 0.002000, GPU Memory: 0.54GB\n",
      "  loss_classifier: 0.0857\n",
      "  loss_box_reg: 0.0565\n",
      "  loss_objectness: 0.4661\n",
      "  loss_rpn_box_reg: 0.8542\n",
      "Epoch [1], Batch [250/1438], Loss: 0.0546, LR: 0.002000, GPU Memory: 0.54GB\n",
      "Epoch [1], Batch [300/1438], Loss: 0.0910, LR: 0.002000, GPU Memory: 0.54GB\n",
      "  loss_classifier: 0.0285\n",
      "  loss_box_reg: 0.0140\n",
      "  loss_objectness: 0.0128\n",
      "  loss_rpn_box_reg: 0.0357\n",
      "Epoch [1], Batch [350/1438], Loss: 0.0935, LR: 0.002000, GPU Memory: 0.54GB\n",
      "Epoch [1], Batch [400/1438], Loss: 0.0554, LR: 0.002000, GPU Memory: 0.54GB\n",
      "  loss_classifier: 0.0138\n",
      "  loss_box_reg: 0.0028\n",
      "  loss_objectness: 0.0149\n",
      "  loss_rpn_box_reg: 0.0239\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify data directory and yaml file exist\n",
    "    if not config.DATA_ROOT.exists():\n",
    "        print(f\"Error: Data directory not found at {config.DATA_ROOT}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    yaml_path = Path(config.DATA_YAML)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"Error: data.yaml not found at {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting Faster R-CNN training for AgroPest-12 dataset\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Classes: {config.NUM_CLASSES}\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_faster_rcnn()\n",
    "    \n",
    "    print(\"\\nTraining completed! You can now:\")\n",
    "    print(\"1. Check 'best_faster_rcnn_model.pth' for the best model\")\n",
    "    print(\"2. View 'faster_rcnn_training_curves.png' for training progress\")\n",
    "    print(\"3. Run inference using the trained model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
