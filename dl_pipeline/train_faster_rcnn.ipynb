{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee04dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "print(\"=== Faster R-CNN Training Setup ===\")\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data configuration from yaml_path\n",
    "# Returns a dict with paths, number of classes, and class names\n",
    "def load_data_config(yaml_path):\n",
    "    yaml_path = Path(yaml_path)\n",
    "    \n",
    "    if not yaml_path.exists():\n",
    "        raise FileNotFoundError(f\"data.yaml not found at {yaml_path}\")\n",
    "    \n",
    "    with open(yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    print(f\"Loaded data configuration from {yaml_path}\")\n",
    "    print(f\"Number of classes: {config['nc']}\")\n",
    "    print(f\"Class names: {config['names']}\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class to store all hyperparameters and settings\n",
    "class Config:\n",
    "    # Path Configuration\n",
    "    DATA_ROOT = Path(\"./data\")\n",
    "    DATA_YAML = \"./data/data.yaml\"\n",
    "    \n",
    "    # Print current working directory for debugging\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "    print(f\"Looking for data.yaml at: {Path(DATA_YAML).absolute()}\")\n",
    "    print(f\"Looking for data root at: {DATA_ROOT.absolute()}\")\n",
    "    \n",
    "    # Training mode selection\n",
    "    # - 'quick_test': Fast iteration for testing pipeline\n",
    "    # - 'full': Complete training on all data\n",
    "    TRAINING_MODE = 'full'\n",
    "    \n",
    "    # Subset configuration based on training mode\n",
    "    if TRAINING_MODE == 'quick_test':\n",
    "        USE_SUBSET = 500\n",
    "        SUBSET_VALID = 100\n",
    "        NUM_EPOCHS = 5\n",
    "        print(f\"\\nâš¡ QUICK TEST MODE: Using {USE_SUBSET} train images, {SUBSET_VALID} valid images, {NUM_EPOCHS} epochs\")\n",
    "    else:\n",
    "        USE_SUBSET = None\n",
    "        SUBSET_VALID = None\n",
    "        NUM_EPOCHS = 12\n",
    "        print(f\"\\nðŸš€ FULL TRAINING MODE: Using all images, {NUM_EPOCHS} epochs\")\n",
    "    \n",
    "    # Load dataset configuration from YAML file\n",
    "    try:\n",
    "        data_config = load_data_config(DATA_YAML)\n",
    "        NUM_CLASSES = data_config['nc'] + 1  # plus 1 for background class\n",
    "        CLASS_NAMES = ['background'] + data_config['names']  # Add background as class 0\n",
    "        \n",
    "        # Verify paths exist\n",
    "        if not DATA_ROOT.exists():\n",
    "            print(f\"Warning: Data root directory not found at {DATA_ROOT.absolute()}\")\n",
    "        else:\n",
    "            print(f\"âœ“ Data root verified at: {DATA_ROOT.absolute()}\")\n",
    "            \n",
    "            # Check train/valid/test directories\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                img_path = DATA_ROOT / split / 'images'\n",
    "                lbl_path = DATA_ROOT / split / 'labels'\n",
    "                if img_path.exists() and lbl_path.exists():\n",
    "                    num_images = len(list(img_path.glob('*.jpg')))\n",
    "                    print(f\"  âœ“ {split}: {num_images} images found\")\n",
    "                else:\n",
    "                    print(f\"  âœ— Warning: {split} directory incomplete or missing\")\n",
    "                    print(f\"    Expected images at: {img_path.absolute()}\")\n",
    "                    print(f\"    Expected labels at: {lbl_path.absolute()}\")\n",
    "                    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nâŒ Error loading data configuration: {e}\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        print(f\"Tried to load from: {Path(DATA_YAML).absolute()}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. You're running the notebook from the dl_pipeline/ directory, OR\")\n",
    "        print(\"  2. Adjust DATA_YAML path to point to your data.yaml location\")\n",
    "        NUM_CLASSES = None\n",
    "        CLASS_NAMES = None\n",
    "    \n",
    "    # Model configuration\n",
    "    BACKBONE = 'resnet50'\n",
    "    PRETRAINED = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 0.002\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    MOMENTUM = 0.9  \n",
    "    \n",
    "    # Image processing\n",
    "    IMG_SIZE = (416, 416)\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Training settings\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_WORKERS = 6\n",
    "    PRINT_FREQ = 5                      # Print training stats every 5 batches\n",
    "    SAVE_FREQ = 2                       # Save checkpoint every 2 epochs\n",
    "    \n",
    "    # Advanced training features\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2     # Simulate batch_size of 8 (4*2)\n",
    "    MIXED_PRECISION = True              # Use FP16 for faster training & less memory\n",
    "    \n",
    "    # Learning rate scheduler settings\n",
    "    LR_STEP_SIZE = 4                    # Reduce LR every 4 epochs\n",
    "    LR_GAMMA = 0.3                      # Multiply LR by 0.3 when stepping\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "print(f\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Hardware: RTX 3070 Ti (8GB VRAM) + i7-12700\")\n",
    "print(f\"Training on device: {config.DEVICE}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "if config.CLASS_NAMES:\n",
    "    print(f\"Class names: {config.CLASS_NAMES}\")\n",
    "print(f\"\\nTraining Settings:\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE} (effective: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS} with gradient accumulation)\")\n",
    "print(f\"  Image size: {config.IMG_SIZE}\")\n",
    "print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  Workers: {config.NUM_WORKERS}\")\n",
    "print(f\"  Mixed precision: {config.MIXED_PRECISION}\")\n",
    "print(f\"\\nEstimated training time:\")\n",
    "if config.TRAINING_MODE == 'quick_test':\n",
    "    print(f\"  âš¡ Quick test: ~30-45 minutes\")\n",
    "else:\n",
    "    print(f\"  ðŸš€ Full training: ~2-3 hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for AgroPest-12 dataset\n",
    "    # This class handles:\n",
    "    #   - Loading images from the dataset\n",
    "    #   - Converting YOLO format annotations to Faster R-CNN format\n",
    "    #   - Applying data augmentations\n",
    "    #   - Returning properly formatted tensors for training\n",
    "class AgroPestDataset(Dataset):\n",
    "    # Initializes the dataset\n",
    "    # Args: \n",
    "    #   - data_dir (str/Path): Root directory containing train/valid/test folders\n",
    "    #   - split (str): Which split to use ('train', 'valid', or 'test')\n",
    "    #   - transform (callable): Optional transform to apply to images\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Set paths for images and labels\n",
    "        self.image_dir = self.data_dir / split / 'images'\n",
    "        self.label_dir = self.data_dir / split / 'labels'\n",
    "        \n",
    "        # Verify that image and label directories exist\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n",
    "        if not self.label_dir.exists():\n",
    "            raise FileNotFoundError(f\"Label directory not found: {self.label_dir}\")\n",
    "        \n",
    "        # Get list of all image files\n",
    "        self.image_paths = list(self.image_dir.glob('*.jpg'))\n",
    "        \n",
    "        # Use subset for faster training/testing if specified\n",
    "        if hasattr(config, 'USE_SUBSET') and config.USE_SUBSET is not None:\n",
    "            if split == 'train' and config.USE_SUBSET:\n",
    "                original_count = len(self.image_paths)\n",
    "                self.image_paths = self.image_paths[:config.USE_SUBSET]\n",
    "                print(f\"Using train subset: {len(self.image_paths)}/{original_count} images\")\n",
    "            elif split == 'valid' and hasattr(config, 'SUBSET_VALID') and config.SUBSET_VALID:\n",
    "                original_count = len(self.image_paths)\n",
    "                self.image_paths = self.image_paths[:config.SUBSET_VALID]\n",
    "                print(f\"Using valid subset: {len(self.image_paths)}/{original_count} images\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.image_paths)} images from {split} split\")\n",
    "    \n",
    "    # Return the total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    # Get a single sample from the dataset\n",
    "    # Args: \n",
    "    #   - idx (int): Index of the sample to retrieve\n",
    "    # Returns: \n",
    "    #   - tuple: (image, target) where image is a tensor and target is a dict\n",
    "    def __getitem__(self, idx):        \n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load annotations\n",
    "        label_path = self.label_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        boxes, labels = self._load_yolo_annotations(label_path, image.size)\n",
    "        \n",
    "        # Prepare target dictionary (format required by Faster R-CNN)\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': self._calculate_area(boxes),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    # Load and convert YOLO format annotations to Faster R-CNN format\n",
    "    # Args:\n",
    "    #   - label_path (Path): Path to the YOLO label file\n",
    "    #   - img_size (tuple): (width, height) of the image\n",
    "    # Returns:\n",
    "    #   - tuple: (boxes, labels) as lists\n",
    "    def _load_yolo_annotations(self, label_path, img_size):\n",
    "        # YOLO format: class_id center_x center_y width height (normalized 0-1)\n",
    "        # Faster R-CNN format: [x_min, y_min, x_max, y_max] (absolute pixels)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        img_width, img_height = img_size\n",
    "        \n",
    "        # Check if label file exists\n",
    "        if not label_path.exists():\n",
    "            # Return empty annotations for images without labels\n",
    "            return [], []\n",
    "        \n",
    "        # Read YOLO format annotations\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "                \n",
    "            # Parse YOLO format: class_id center_x center_y width height\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip malformed lines\n",
    "                \n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1])\n",
    "            center_y = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            # Convert normalized coordinates to absolute coordinates\n",
    "            center_x_abs = center_x * img_width\n",
    "            center_y_abs = center_y * img_height\n",
    "            width_abs = width * img_width\n",
    "            height_abs = height * img_height\n",
    "            \n",
    "            # Convert center format to corner format\n",
    "            x_min = center_x_abs - width_abs / 2\n",
    "            y_min = center_y_abs - height_abs / 2\n",
    "            x_max = center_x_abs + width_abs / 2\n",
    "            y_max = center_y_abs + height_abs / 2\n",
    "            \n",
    "            # Ensure coordinates are within image bounds\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(img_width, x_max)\n",
    "            y_max = min(img_height, y_max)\n",
    "            \n",
    "            # Only add valid boxes (with positive area)\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(class_id + 1)  # Add 1 because Faster R-CNN uses 1-indexed labels (0 is background)\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    # Calculate area of bounding boxes\n",
    "    def _calculate_area(self, boxes):\n",
    "        if len(boxes) == 0:\n",
    "            return torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        return (boxes_tensor[:, 2] - boxes_tensor[:, 0]) * (boxes_tensor[:, 3] - boxes_tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data transforms for training or validation\n",
    "# Args:\n",
    "#  - split (str): 'train' for training transforms, anything else for validation\n",
    "# Returns:\n",
    "#  - torchvision.transforms.Compose: Composed transforms\n",
    "def get_transforms(split='train'):\n",
    "    if split == 'train':\n",
    "        # Training transforms with data augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color augmentation\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])\n",
    "    else:\n",
    "        # Validation transforms without augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05328bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Faster R-CNN model with custom number of classes\n",
    "# Args:\n",
    "#   - num_classes (int): Number of classes including background\n",
    "# Returns:\n",
    "#   - torch.nn.Module: Faster R-CNN model\n",
    "def create_faster_rcnn_model(num_classes):\n",
    "    # How Faster R-CNN works:\n",
    "    # 1. Backbone (ResNet-50): Extracts features from input image\n",
    "    # 2. FPN (Feature Pyramid Network): Combines features at different scales\n",
    "    # 3. RPN (Region Proposal Network): Generates potential object locations\n",
    "    # 4. ROI Head: Classifies proposals and refines bounding boxes\n",
    "    \n",
    "    # Load pre-trained Faster R-CNN model with ResNet-50 backbone and FPN\n",
    "    # This model is pre-trained on COCO dataset (80 classes + background)\n",
    "    from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "    if config.PRETRAINED:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    # The classifier head takes features from the ROI pooling layer\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one for our number of classes\n",
    "    # FastRCNNPredictor includes both classification and bounding box regression\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    print(f\"Created Faster R-CNN model with {num_classes} classes\")\n",
    "    print(f\"Backbone: ResNet-50 + FPN\")\n",
    "    print(f\"Classifier input features: {in_features}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function to handle variable number of objects per image\n",
    "# Args:\n",
    "#  - batch (list): List of (image, target) tuples\n",
    "# Returns:\n",
    "#   - tuple: (images, targets) as lists\n",
    "def collate_fn(batch):\n",
    "    # Faster R-CNN needs a special collate function because:\n",
    "    # - Different images have different numbers of objects\n",
    "    # - We can't stack tensors with different shapes\n",
    "    # - We need to return lists of images and targets\n",
    "\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for one epoch\n",
    "# Args:\n",
    "#   - model: The Faster R-CNN model\n",
    "#   - optimizer: Optimizer for updating model weights\n",
    "#   - data_loader: DataLoader for training data\n",
    "#   - device: Device to run training on (CPU/GPU)\n",
    "#   - epoch: Current epoch number (for logging)\n",
    "#   - print_freq: Frequency of printing training stats\n",
    "# Returns:\n",
    "#   - float: Average loss for the epoch\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=5):\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize mixed precision scaler\n",
    "    if config.MIXED_PRECISION:\n",
    "        try:\n",
    "            scaler = GradScaler(\"cuda\")\n",
    "        except TypeError:\n",
    "            scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Move images and targets to device (GPU)\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass with mixed precision if enabled\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            try:\n",
    "                with autocast(\"cuda\"):\n",
    "                    loss_dict = model(images, targets)\n",
    "            except AssertionError as e:\n",
    "                if \"Expected target boxes to be a tensor\" in str(e):\n",
    "                    print(f\"Skipping batch with empty annotations\")\n",
    "                    continue  # Skip this batch\n",
    "                else:\n",
    "                    raise e  # Re-raise if it's a different error\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        total_loss = total_loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            scaler.scale(total_loss).backward()  # Scaled backward pass\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        losses.append(total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS)  # Store unscaled loss\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "        if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if config.MIXED_PRECISION and scaler is not None:\n",
    "                scaler.step(optimizer)  # Update weights with scaling\n",
    "                scaler.update()  # Update scaler for next iteration\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "        if batch_idx % print_freq == 0 or batch_idx == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "            print(f'Epoch [{epoch}], Batch [{batch_idx}/{len(data_loader)}], '\n",
    "                  f'Loss: {total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS:.4f}, '\n",
    "                  f'LR: {current_lr:.6f}, GPU Memory: {memory_used:.2f}GB')\n",
    "            \n",
    "            # Print individual loss components for debugging (less frequently)\n",
    "            if batch_idx % (print_freq * 2) == 0:\n",
    "                for loss_name, loss_value in loss_dict.items():\n",
    "                    print(f'  {loss_name}: {loss_value.item():.4f}')\n",
    "        \n",
    "        # Clear cache periodically to prevent memory fragmentation\n",
    "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "# Validate the model (calculate validation loss)\n",
    "# Args:\n",
    "#   - model: Trained model\n",
    "#   - data_loader: DataLoader with validation data\n",
    "#   - device: Device to run validation on\n",
    "# Returns:\n",
    "#   - float: Average validation loss\n",
    "def validate_model(model, data_loader, device):\n",
    "    model.train()  # Keep in training mode to get loss values\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for images, targets in data_loader:\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.append(total_loss.item())\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main training function to train the Faster R-CNN model\n",
    "def train_faster_rcnn():\n",
    "    # Create datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT, \n",
    "        split='train',\n",
    "        transform=get_transforms('train')\n",
    "    )\n",
    "    \n",
    "    valid_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT,\n",
    "        split='valid', \n",
    "        transform=get_transforms('valid')\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,           # Shuffle training data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn   # Use custom collate function\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,          # Don't shuffle validation data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Valid batches: {len(valid_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_faster_rcnn_model(config.NUM_CLASSES)\n",
    "    model.to(config.DEVICE)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        momentum=config.MOMENTUM,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.LR_STEP_SIZE,\n",
    "        gamma=config.LR_GAMMA\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {config.NUM_EPOCHS} epochs...\")\n",
    "    print(f\"  - Batch size: {config.BATCH_SIZE} (with gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS})\")\n",
    "    print(f\"  - Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - Image size: {config.IMG_SIZE}\")\n",
    "    print(f\"  - Mixed precision: {config.MIXED_PRECISION}\")\n",
    "    print(f\"  - Memory monitoring enabled\")\n",
    "    \n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  - Initial GPU memory: {initial_memory:.2f}GB / 4.3GB\")\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{config.NUM_EPOCHS} ===\")\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, config.DEVICE, epoch + 1, config.PRINT_FREQ\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        valid_loss = validate_model(model, valid_loader, config.DEVICE)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Memory monitoring\n",
    "        if torch.cuda.is_available():\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            torch.cuda.reset_peak_memory_stats()  # Reset for next epoch\n",
    "        else:\n",
    "            peak_memory = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.1f}s, Peak GPU Memory: {peak_memory:.2f}GB\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Memory warning\n",
    "        if peak_memory > 4.0:\n",
    "            print(f\"    WARNING: High memory usage ({peak_memory:.2f}GB). Consider reducing batch size or image size.\")\n",
    "        \n",
    "        # Save best model\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__  # Save all config settings\n",
    "            }, 'best_faster_rcnn_model.pth')\n",
    "            print(f\"  âœ… New best model saved! Validation loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every few epochs\n",
    "        if (epoch + 1) % config.SAVE_FREQ == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__\n",
    "            }, f'faster_rcnn_checkpoint_epoch_{epoch + 1}.pth')\n",
    "    \n",
    "    # Plot training curves    \n",
    "    plt.close('all')\n",
    "    \n",
    "    # Create plot with error handling\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(train_losses, label='Training Loss', marker='o')\n",
    "    ax.plot(valid_losses, label='Validation Loss', marker='s')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Faster R-CNN Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png', dpi=100, bbox_inches='tight')\n",
    "    print(f\"âœ… Training curves saved in '../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png'\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275af32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify data directory and yaml file exist\n",
    "    if not config.DATA_ROOT.exists():\n",
    "        print(f\"Error: Data directory not found at {config.DATA_ROOT}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    yaml_path = Path(config.DATA_YAML)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"Error: data.yaml not found at {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting Faster R-CNN training for AgroPest-12 dataset\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Classes: {config.NUM_CLASSES}\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_faster_rcnn()\n",
    "    \n",
    "    print(\"\\nTraining completed! You can now:\")\n",
    "    print(\"1. Check 'best_faster_rcnn_model.pth' for the best model\")\n",
    "    print(\"2. View 'faster_rcnn_training_curves.png' for training progress\")\n",
    "    print(\"3. Run inference using the trained model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
