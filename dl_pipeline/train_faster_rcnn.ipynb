{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee04dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Faster R-CNN Training Setup ===\n",
      "Python version: 3.10.12\n",
      "PyTorch version: 2.5.1+cu124\n",
      "Torchvision version: 0.20.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "CUDA memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import yaml\n",
    "\n",
    "print(\"=== Faster R-CNN Training Setup ===\")\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data configuration from yaml_path\n",
    "# Returns a dict with paths, number of classes, and class names\n",
    "def load_data_config(yaml_path):\n",
    "    yaml_path = Path(yaml_path)\n",
    "    \n",
    "    if not yaml_path.exists():\n",
    "        raise FileNotFoundError(f\"data.yaml not found at {yaml_path}\")\n",
    "    \n",
    "    with open(yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    print(f\"Loaded data configuration from {yaml_path}\")\n",
    "    print(f\"Number of classes: {config['nc']}\")\n",
    "    print(f\"Class names: {config['names']}\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7396ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/brendan/comp9517/COMP9517-group-project\n",
      "Looking for data.yaml at: /home/brendan/comp9517/COMP9517-group-project/data/data.yaml\n",
      "Looking for data root at: /home/brendan/comp9517/COMP9517-group-project/data\n",
      "Loaded data configuration from data/data.yaml\n",
      "Number of classes: 12\n",
      "Class names: ['Ants', 'Bees', 'Beetles', 'Caterpillars', 'Earthworms', 'Earwigs', 'Grasshoppers', 'Moths', 'Slugs', 'Snails', 'Wasps', 'Weevils']\n",
      "✓ Data root verified at: /home/brendan/comp9517/COMP9517-group-project/data\n",
      "  ✓ train: 11502 images found\n",
      "  ✓ valid: 1095 images found\n",
      "  ✓ test: 546 images found\n",
      "\n",
      "=== Configuration Summary ===\n",
      "Training on device: cuda\n",
      "Number of classes: 13\n",
      "Class names: ['background', 'Ants', 'Bees', 'Beetles', 'Caterpillars', 'Earthworms', 'Earwigs', 'Grasshoppers', 'Moths', 'Slugs', 'Snails', 'Wasps', 'Weevils']\n"
     ]
    }
   ],
   "source": [
    "# Configuration class to store all hyperparameters and settings\n",
    "class Config:\n",
    "    # Path configuration - use relative paths from notebook location\n",
    "    # The notebook is in dl_pipeline/, data is in ../data/\n",
    "    DATA_ROOT = Path(\"./data\")\n",
    "    DATA_YAML = \"./data/data.yaml\"\n",
    "    \n",
    "    # Print current working directory for debugging\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "    print(f\"Looking for data.yaml at: {Path(DATA_YAML).absolute()}\")\n",
    "    print(f\"Looking for data root at: {DATA_ROOT.absolute()}\")\n",
    "    \n",
    "    # Subset configuration for faster testing\n",
    "    USE_SUBSET = None  # Set to a number like 100 to use only N training images\n",
    "    SUBSET_VALID = None  # Set to a number to limit validation images\n",
    "    \n",
    "    # Load dataset configuration from YAML file\n",
    "    try:\n",
    "        data_config = load_data_config(DATA_YAML)\n",
    "        NUM_CLASSES = data_config['nc'] + 1  # plus 1 for background class\n",
    "        CLASS_NAMES = ['background'] + data_config['names']  # Add background as class 0\n",
    "        \n",
    "        # Verify paths exist\n",
    "        if not DATA_ROOT.exists():\n",
    "            print(f\"Warning: Data root directory not found at {DATA_ROOT.absolute()}\")\n",
    "        else:\n",
    "            print(f\"✓ Data root verified at: {DATA_ROOT.absolute()}\")\n",
    "            \n",
    "            # Check train/valid/test directories\n",
    "            for split in ['train', 'valid', 'test']:\n",
    "                img_path = DATA_ROOT / split / 'images'\n",
    "                lbl_path = DATA_ROOT / split / 'labels'\n",
    "                if img_path.exists() and lbl_path.exists():\n",
    "                    num_images = len(list(img_path.glob('*.jpg')))\n",
    "                    print(f\"  ✓ {split}: {num_images} images found\")\n",
    "                else:\n",
    "                    print(f\"  ✗ Warning: {split} directory incomplete or missing\")\n",
    "                    print(f\"    Expected images at: {img_path.absolute()}\")\n",
    "                    print(f\"    Expected labels at: {lbl_path.absolute()}\")\n",
    "                    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n❌ Error loading data configuration: {e}\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "        print(f\"Tried to load from: {Path(DATA_YAML).absolute()}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. You're running the notebook from the dl_pipeline/ directory, OR\")\n",
    "        print(\"  2. Adjust DATA_YAML path to point to your data.yaml location\")\n",
    "        NUM_CLASSES = None\n",
    "        CLASS_NAMES = None\n",
    "    \n",
    "    # Model configuration\n",
    "    BACKBONE = 'resnet50'\n",
    "    PRETRAINED = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 0.002\n",
    "    NUM_EPOCHS = 5\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    MOMENTUM = 0.9\n",
    "    \n",
    "    # Image processing\n",
    "    IMG_SIZE = (416, 416)  # Input image size (height, width)\n",
    "    MEAN = [0.485, 0.456, 0.406]  # ImageNet mean\n",
    "    STD = [0.229, 0.224, 0.225]   # ImageNet std\n",
    "    \n",
    "    # Training settings\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_WORKERS = 4\n",
    "    PRINT_FREQ = 2\n",
    "    SAVE_FREQ = 1\n",
    "    \n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    # Learning rate scheduler settings\n",
    "    LR_STEP_SIZE = 2\n",
    "    LR_GAMMA = 0.3\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "print(f\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Training on device: {config.DEVICE}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "if config.CLASS_NAMES:\n",
    "    print(f\"Class names: {config.CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc6e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DATASET CLASS FOR LOADING AGROPEST DATA\n",
    "class AgroPestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for loading AgroPest-12 data\n",
    "    \n",
    "    This class handles:\n",
    "    - Loading images from the dataset\n",
    "    - Converting YOLO format annotations to Faster R-CNN format\n",
    "    - Applying data augmentations\n",
    "    - Returning properly formatted tensors for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str/Path): Root directory containing train/valid/test folders\n",
    "            split (str): Which split to use ('train', 'valid', or 'test')\n",
    "            transform (callable): Optional transform to apply to images\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Set paths for images and labels\n",
    "        # Structure: data_dir/split/images and data_dir/split/labels\n",
    "        self.image_dir = self.data_dir / split / 'images'\n",
    "        self.label_dir = self.data_dir / split / 'labels'\n",
    "        \n",
    "        # Verify that image and label directories exist\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n",
    "        if not self.label_dir.exists():\n",
    "            raise FileNotFoundError(f\"Label directory not found: {self.label_dir}\")\n",
    "        \n",
    "        # Get list of all image files\n",
    "        self.image_paths = list(self.image_dir.glob('*.jpg'))\n",
    "        \n",
    "        # Use subset for faster training/testing if specified\n",
    "        if hasattr(config, 'USE_SUBSET') and config.USE_SUBSET is not None:\n",
    "            if split == 'train' and config.USE_SUBSET:\n",
    "                original_count = len(self.image_paths)\n",
    "                self.image_paths = self.image_paths[:config.USE_SUBSET]\n",
    "                print(f\"Using train subset: {len(self.image_paths)}/{original_count} images\")\n",
    "            elif split == 'valid' and hasattr(config, 'SUBSET_VALID') and config.SUBSET_VALID:\n",
    "                original_count = len(self.image_paths)\n",
    "                self.image_paths = self.image_paths[:config.SUBSET_VALID]\n",
    "                print(f\"Using valid subset: {len(self.image_paths)}/{original_count} images\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.image_paths)} images from {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, target) where image is a tensor and target is a dict\n",
    "        \"\"\"\n",
    "        \n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load annotations - label file has same stem as image\n",
    "        label_path = self.label_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        boxes, labels = self._load_yolo_annotations(label_path, image.size)\n",
    "        \n",
    "        # Prepare target dictionary (format required by Faster R-CNN)\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': self._calculate_area(boxes),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def _load_yolo_annotations(self, label_path, img_size):\n",
    "        \"\"\"\n",
    "        Load and convert YOLO format annotations to Faster R-CNN format\n",
    "        \n",
    "        YOLO format: class_id center_x center_y width height (normalized 0-1)\n",
    "        Faster R-CNN format: [x_min, y_min, x_max, y_max] (absolute pixels)\n",
    "        \n",
    "        Args:\n",
    "            label_path (Path): Path to the YOLO label file\n",
    "            img_size (tuple): (width, height) of the image\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (boxes, labels) as lists\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        img_width, img_height = img_size\n",
    "        \n",
    "        # Check if label file exists (some images might not have annotations)\n",
    "        if not label_path.exists():\n",
    "            # Return empty annotations for images without labels\n",
    "            return [], []\n",
    "        \n",
    "        # Read YOLO format annotations\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "                \n",
    "            # Parse YOLO format: class_id center_x center_y width height\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip malformed lines\n",
    "                \n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1])\n",
    "            center_y = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            \n",
    "            # Convert normalized coordinates to absolute coordinates\n",
    "            center_x_abs = center_x * img_width\n",
    "            center_y_abs = center_y * img_height\n",
    "            width_abs = width * img_width\n",
    "            height_abs = height * img_height\n",
    "            \n",
    "            # Convert center format to corner format\n",
    "            x_min = center_x_abs - width_abs / 2\n",
    "            y_min = center_y_abs - height_abs / 2\n",
    "            x_max = center_x_abs + width_abs / 2\n",
    "            y_max = center_y_abs + height_abs / 2\n",
    "            \n",
    "            # Ensure coordinates are within image bounds\n",
    "            x_min = max(0, x_min)\n",
    "            y_min = max(0, y_min)\n",
    "            x_max = min(img_width, x_max)\n",
    "            y_max = min(img_height, y_max)\n",
    "            \n",
    "            # Only add valid boxes (with positive area)\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(class_id + 1)  # Add 1 because Faster R-CNN uses 1-indexed labels (0 is background)\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def _calculate_area(self, boxes):\n",
    "        \"\"\"Calculate area of bounding boxes\"\"\"\n",
    "        if len(boxes) == 0:\n",
    "            return torch.zeros((0,), dtype=torch.float32)\n",
    "        \n",
    "        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        return (boxes_tensor[:, 2] - boxes_tensor[:, 0]) * (boxes_tensor[:, 3] - boxes_tensor[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfdc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. DATA TRANSFORMS AND AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_transforms(split='train'):\n",
    "    \"\"\"\n",
    "    Get data transforms for training or validation\n",
    "    \n",
    "    Args:\n",
    "        split (str): 'train' for training transforms, anything else for validation\n",
    "        \n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Composed transforms\n",
    "    \"\"\"\n",
    "    if split == 'train':\n",
    "        # Training transforms with data augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color augmentation\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])\n",
    "    else:\n",
    "        # Validation transforms without augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(config.IMG_SIZE),  # Resize to fixed size\n",
    "            transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "            transforms.Normalize(mean=config.MEAN, std=config.STD)  # Normalize with ImageNet stats\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05328bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. MODEL CREATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def create_faster_rcnn_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create a Faster R-CNN model with custom number of classes\n",
    "    \n",
    "    How Faster R-CNN works:\n",
    "    1. Backbone (ResNet-50): Extracts features from input image\n",
    "    2. FPN (Feature Pyramid Network): Combines features at different scales\n",
    "    3. RPN (Region Proposal Network): Generates potential object locations\n",
    "    4. ROI Head: Classifies proposals and refines bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of classes including background\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Faster R-CNN model\n",
    "    \"\"\"\n",
    "    # Load pre-trained Faster R-CNN model with ResNet-50 backbone and FPN\n",
    "    # This model is pre-trained on COCO dataset (80 classes + background)\n",
    "    from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "    if config.PRETRAINED:\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    # The classifier head takes features from the ROI pooling layer\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one for our number of classes\n",
    "    # FastRCNNPredictor includes both classification and bounding box regression\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    print(f\"Created Faster R-CNN model with {num_classes} classes\")\n",
    "    print(f\"Backbone: ResNet-50 + FPN\")\n",
    "    print(f\"Classifier input features: {in_features}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6642a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. COLLATE FUNCTION FOR DATALOADER\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per image\n",
    "    \n",
    "    Faster R-CNN needs a special collate function because:\n",
    "    - Different images have different numbers of objects\n",
    "    - We can't stack tensors with different shapes\n",
    "    - We need to return lists of images and targets\n",
    "    \n",
    "    Args:\n",
    "        batch (list): List of (image, target) tuples\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (images, targets) as lists\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afbd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=5):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize mixed precision scaler\n",
    "    if config.MIXED_PRECISION:\n",
    "        try:\n",
    "            scaler = GradScaler(\"cuda\")\n",
    "        except TypeError:\n",
    "            scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Move images and targets to device (GPU)\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass with mixed precision if enabled\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            try:\n",
    "                with autocast(\"cuda\"):\n",
    "                    loss_dict = model(images, targets)\n",
    "            except AssertionError as e:\n",
    "                if \"Expected target boxes to be a tensor\" in str(e):\n",
    "                    print(f\"Skipping batch with empty annotations\")\n",
    "                    continue  # Skip this batch\n",
    "                else:\n",
    "                    raise e  # Re-raise if it's a different error\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        total_loss = total_loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        if config.MIXED_PRECISION and scaler is not None:\n",
    "            scaler.scale(total_loss).backward()  # Scaled backward pass\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        losses.append(total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS)  # Store unscaled loss\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "        if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if config.MIXED_PRECISION and scaler is not None:\n",
    "                scaler.step(optimizer)  # Update weights with scaling\n",
    "                scaler.update()  # Update scaler for next iteration\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "        if batch_idx % print_freq == 0 or batch_idx == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "            print(f'Epoch [{epoch}], Batch [{batch_idx}/{len(data_loader)}], '\n",
    "                  f'Loss: {total_loss.item() * config.GRADIENT_ACCUMULATION_STEPS:.4f}, '\n",
    "                  f'LR: {current_lr:.6f}, GPU Memory: {memory_used:.2f}GB')\n",
    "            \n",
    "            # Print individual loss components for debugging (less frequently)\n",
    "            if batch_idx % (print_freq * 2) == 0:\n",
    "                for loss_name, loss_value in loss_dict.items():\n",
    "                    print(f'  {loss_name}: {loss_value.item():.4f}')\n",
    "        \n",
    "        # Clear cache periodically to prevent memory fragmentation\n",
    "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def validate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the model (calculate validation loss)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader with validation data\n",
    "        device: Device to run validation on\n",
    "        \n",
    "    Returns:\n",
    "        float: Average validation loss\n",
    "    \"\"\"\n",
    "    model.train()  # Keep in training mode to get loss values\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for images, targets in data_loader:\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.append(total_loss.item())\n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6aa6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 9. MAIN TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_faster_rcnn():\n",
    "    \"\"\"\n",
    "    Main function to train the Faster R-CNN model\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT, \n",
    "        split='train',\n",
    "        transform=get_transforms('train')\n",
    "    )\n",
    "    \n",
    "    valid_dataset = AgroPestDataset(\n",
    "        config.DATA_ROOT,\n",
    "        split='valid', \n",
    "        transform=get_transforms('valid')\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,  # Shuffle training data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn  # Use custom collate function\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,  # Don't shuffle validation data\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Valid batches: {len(valid_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_faster_rcnn_model(config.NUM_CLASSES)\n",
    "    model.to(config.DEVICE)\n",
    "    \n",
    "    # Create optimizer\n",
    "    # Using SGD with momentum (common choice for object detection)\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        momentum=config.MOMENTUM,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (reduce LR when training stagnates)\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config.LR_STEP_SIZE,  # Use config value\n",
    "        gamma=config.LR_GAMMA          # Use config value\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {config.NUM_EPOCHS} epochs...\")\n",
    "    print(f\"  - Batch size: {config.BATCH_SIZE} (with gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS})\")\n",
    "    print(f\"  - Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - Image size: {config.IMG_SIZE}\")\n",
    "    print(f\"  - Mixed precision: {config.MIXED_PRECISION}\")\n",
    "    print(f\"  - Memory monitoring enabled\")\n",
    "    \n",
    "    # Clear GPU cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  - Initial GPU memory: {initial_memory:.2f}GB / 4.3GB\")\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{config.NUM_EPOCHS} ===\")\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, config.DEVICE, epoch + 1, config.PRINT_FREQ\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        valid_loss = validate_model(model, valid_loader, config.DEVICE)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        # Memory monitoring for RTX 3050 Ti\n",
    "        if torch.cuda.is_available():\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "            torch.cuda.reset_peak_memory_stats()  # Reset for next epoch\n",
    "        else:\n",
    "            peak_memory = current_memory = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.1f}s, Peak GPU Memory: {peak_memory:.2f}GB\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Memory warning\n",
    "        if peak_memory > 4.0:\n",
    "            print(f\"    WARNING: High memory usage ({peak_memory:.2f}GB). Consider reducing batch size or image size.\")\n",
    "        \n",
    "        # Save best model\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__  # Save all config settings\n",
    "            }, 'best_faster_rcnn_model.pth')\n",
    "            print(f\"  ✅ New best model saved! Validation loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint every few epochs\n",
    "        if (epoch + 1) % config.SAVE_FREQ == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'config': config.__dict__\n",
    "            }, f'faster_rcnn_checkpoint_epoch_{epoch + 1}.pth')\n",
    "    \n",
    "    # Plot training curves    \n",
    "    plt.close('all')\n",
    "    \n",
    "    # Create plot with error handling\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(train_losses, label='Training Loss', marker='o')\n",
    "    ax.plot(valid_losses, label='Validation Loss', marker='s')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Faster R-CNN Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png', dpi=100, bbox_inches='tight')\n",
    "    print(f\"✅ Training curves saved in '../results/faster_rcnn_evaluation/faster_rcnn_training_curves.png'\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275af32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Faster R-CNN training for AgroPest-12 dataset\n",
      "Configuration:\n",
      "  Classes: 13\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.002\n",
      "  Epochs: 5\n",
      "  Device: cuda\n",
      "Loading datasets...\n",
      "Loaded 11502 images from train split\n",
      "Loaded 1095 images from valid split\n",
      "Train batches: 1438\n",
      "Valid batches: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/brendan/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████████████████████████████████████████████████| 160M/160M [00:09<00:00, 18.0MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Faster R-CNN model with 13 classes\n",
      "Backbone: ResNet-50 + FPN\n",
      "Classifier input features: 1024\n",
      "Starting training for 5 epochs...\n",
      "  - Batch size: 8 (with gradient accumulation: 1)\n",
      "  - Effective batch size: 8\n",
      "  - Image size: (416, 416)\n",
      "  - Mixed precision: True\n",
      "  - Memory monitoring enabled\n",
      "  - Initial GPU memory: 0.17GB / 4.3GB\n",
      "\n",
      "=== Epoch 1/5 ===\n",
      "Starting training for 5 epochs...\n",
      "  - Batch size: 8 (with gradient accumulation: 1)\n",
      "  - Effective batch size: 8\n",
      "  - Image size: (416, 416)\n",
      "  - Mixed precision: True\n",
      "  - Memory monitoring enabled\n",
      "  - Initial GPU memory: 0.17GB / 4.3GB\n",
      "\n",
      "=== Epoch 1/5 ===\n",
      "Epoch [1], Batch [0/1438], Loss: 3.1680, LR: 0.002000, GPU Memory: 0.20GB\n",
      "  loss_classifier: 2.8613\n",
      "  loss_box_reg: 0.0231\n",
      "  loss_objectness: 0.2028\n",
      "  loss_rpn_box_reg: 0.0809\n",
      "Epoch [1], Batch [0/1438], Loss: 3.1680, LR: 0.002000, GPU Memory: 0.20GB\n",
      "  loss_classifier: 2.8613\n",
      "  loss_box_reg: 0.0231\n",
      "  loss_objectness: 0.2028\n",
      "  loss_rpn_box_reg: 0.0809\n",
      "Epoch [1], Batch [2/1438], Loss: 3.2145, LR: 0.002000, GPU Memory: 0.20GB\n",
      "Epoch [1], Batch [2/1438], Loss: 3.2145, LR: 0.002000, GPU Memory: 0.20GB\n",
      "Epoch [1], Batch [4/1438], Loss: 1.2688, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.7971\n",
      "  loss_box_reg: 0.0021\n",
      "  loss_objectness: 0.2489\n",
      "  loss_rpn_box_reg: 0.2207\n",
      "Epoch [1], Batch [4/1438], Loss: 1.2688, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.7971\n",
      "  loss_box_reg: 0.0021\n",
      "  loss_objectness: 0.2489\n",
      "  loss_rpn_box_reg: 0.2207\n",
      "Epoch [1], Batch [6/1438], Loss: 0.1672, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [6/1438], Loss: 0.1672, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [8/1438], Loss: 0.4164, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.2200\n",
      "  loss_box_reg: 0.0393\n",
      "  loss_objectness: 0.1107\n",
      "  loss_rpn_box_reg: 0.0465\n",
      "Epoch [1], Batch [8/1438], Loss: 0.4164, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.2200\n",
      "  loss_box_reg: 0.0393\n",
      "  loss_objectness: 0.1107\n",
      "  loss_rpn_box_reg: 0.0465\n",
      "Epoch [1], Batch [10/1438], Loss: 0.2749, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [10/1438], Loss: 0.2749, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [12/1438], Loss: 0.1757, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0796\n",
      "  loss_box_reg: 0.0161\n",
      "  loss_objectness: 0.0093\n",
      "  loss_rpn_box_reg: 0.0708\n",
      "Epoch [1], Batch [12/1438], Loss: 0.1757, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0796\n",
      "  loss_box_reg: 0.0161\n",
      "  loss_objectness: 0.0093\n",
      "  loss_rpn_box_reg: 0.0708\n",
      "Epoch [1], Batch [14/1438], Loss: 0.1904, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [14/1438], Loss: 0.1904, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [16/1438], Loss: 0.1985, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0956\n",
      "  loss_box_reg: 0.0225\n",
      "  loss_objectness: 0.0260\n",
      "  loss_rpn_box_reg: 0.0544\n",
      "Epoch [1], Batch [16/1438], Loss: 0.1985, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0956\n",
      "  loss_box_reg: 0.0225\n",
      "  loss_objectness: 0.0260\n",
      "  loss_rpn_box_reg: 0.0544\n",
      "Epoch [1], Batch [18/1438], Loss: 0.2178, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [18/1438], Loss: 0.2178, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [20/1438], Loss: 0.2189, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1069\n",
      "  loss_box_reg: 0.0242\n",
      "  loss_objectness: 0.0453\n",
      "  loss_rpn_box_reg: 0.0425\n",
      "Epoch [1], Batch [20/1438], Loss: 0.2189, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1069\n",
      "  loss_box_reg: 0.0242\n",
      "  loss_objectness: 0.0453\n",
      "  loss_rpn_box_reg: 0.0425\n",
      "Epoch [1], Batch [22/1438], Loss: 0.1424, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [22/1438], Loss: 0.1424, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [24/1438], Loss: 0.5149, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.2642\n",
      "  loss_box_reg: 0.0277\n",
      "  loss_objectness: 0.1442\n",
      "  loss_rpn_box_reg: 0.0788\n",
      "Epoch [1], Batch [24/1438], Loss: 0.5149, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.2642\n",
      "  loss_box_reg: 0.0277\n",
      "  loss_objectness: 0.1442\n",
      "  loss_rpn_box_reg: 0.0788\n",
      "Epoch [1], Batch [26/1438], Loss: 0.1257, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [26/1438], Loss: 0.1257, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [28/1438], Loss: 0.2049, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1144\n",
      "  loss_box_reg: 0.0238\n",
      "  loss_objectness: 0.0348\n",
      "  loss_rpn_box_reg: 0.0320\n",
      "Epoch [1], Batch [28/1438], Loss: 0.2049, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1144\n",
      "  loss_box_reg: 0.0238\n",
      "  loss_objectness: 0.0348\n",
      "  loss_rpn_box_reg: 0.0320\n",
      "Epoch [1], Batch [30/1438], Loss: 0.2551, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [30/1438], Loss: 0.2551, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [32/1438], Loss: 0.2103, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0958\n",
      "  loss_box_reg: 0.0373\n",
      "  loss_objectness: 0.0329\n",
      "  loss_rpn_box_reg: 0.0443\n",
      "Epoch [1], Batch [32/1438], Loss: 0.2103, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0958\n",
      "  loss_box_reg: 0.0373\n",
      "  loss_objectness: 0.0329\n",
      "  loss_rpn_box_reg: 0.0443\n",
      "Epoch [1], Batch [34/1438], Loss: 0.1218, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [34/1438], Loss: 0.1218, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [36/1438], Loss: 0.1516, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0603\n",
      "  loss_box_reg: 0.0283\n",
      "  loss_objectness: 0.0178\n",
      "  loss_rpn_box_reg: 0.0452\n",
      "Epoch [1], Batch [36/1438], Loss: 0.1516, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0603\n",
      "  loss_box_reg: 0.0283\n",
      "  loss_objectness: 0.0178\n",
      "  loss_rpn_box_reg: 0.0452\n",
      "Epoch [1], Batch [38/1438], Loss: 0.1552, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [38/1438], Loss: 0.1552, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [40/1438], Loss: 0.1093, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0381\n",
      "  loss_box_reg: 0.0161\n",
      "  loss_objectness: 0.0146\n",
      "  loss_rpn_box_reg: 0.0405\n",
      "Epoch [1], Batch [40/1438], Loss: 0.1093, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0381\n",
      "  loss_box_reg: 0.0161\n",
      "  loss_objectness: 0.0146\n",
      "  loss_rpn_box_reg: 0.0405\n",
      "Epoch [1], Batch [42/1438], Loss: 0.1090, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [42/1438], Loss: 0.1090, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [44/1438], Loss: 0.0919, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0299\n",
      "  loss_box_reg: 0.0119\n",
      "  loss_objectness: 0.0170\n",
      "  loss_rpn_box_reg: 0.0331\n",
      "Epoch [1], Batch [44/1438], Loss: 0.0919, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0299\n",
      "  loss_box_reg: 0.0119\n",
      "  loss_objectness: 0.0170\n",
      "  loss_rpn_box_reg: 0.0331\n",
      "Epoch [1], Batch [46/1438], Loss: 0.1057, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [46/1438], Loss: 0.1057, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [48/1438], Loss: 0.1264, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0209\n",
      "  loss_box_reg: 0.0109\n",
      "  loss_objectness: 0.0407\n",
      "  loss_rpn_box_reg: 0.0539\n",
      "Epoch [1], Batch [48/1438], Loss: 0.1264, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0209\n",
      "  loss_box_reg: 0.0109\n",
      "  loss_objectness: 0.0407\n",
      "  loss_rpn_box_reg: 0.0539\n",
      "Epoch [1], Batch [50/1438], Loss: 0.0971, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [50/1438], Loss: 0.0971, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [52/1438], Loss: 0.1127, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0333\n",
      "  loss_box_reg: 0.0132\n",
      "  loss_objectness: 0.0247\n",
      "  loss_rpn_box_reg: 0.0415\n",
      "Epoch [1], Batch [52/1438], Loss: 0.1127, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0333\n",
      "  loss_box_reg: 0.0132\n",
      "  loss_objectness: 0.0247\n",
      "  loss_rpn_box_reg: 0.0415\n",
      "Epoch [1], Batch [54/1438], Loss: 0.1133, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [54/1438], Loss: 0.1133, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [56/1438], Loss: 0.1172, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0231\n",
      "  loss_box_reg: 0.0100\n",
      "  loss_objectness: 0.0178\n",
      "  loss_rpn_box_reg: 0.0663\n",
      "Epoch [1], Batch [56/1438], Loss: 0.1172, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0231\n",
      "  loss_box_reg: 0.0100\n",
      "  loss_objectness: 0.0178\n",
      "  loss_rpn_box_reg: 0.0663\n",
      "Epoch [1], Batch [58/1438], Loss: 0.1469, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [58/1438], Loss: 0.1469, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [60/1438], Loss: 0.1038, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0207\n",
      "  loss_box_reg: 0.0097\n",
      "  loss_objectness: 0.0143\n",
      "  loss_rpn_box_reg: 0.0590\n",
      "Epoch [1], Batch [60/1438], Loss: 0.1038, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0207\n",
      "  loss_box_reg: 0.0097\n",
      "  loss_objectness: 0.0143\n",
      "  loss_rpn_box_reg: 0.0590\n",
      "Epoch [1], Batch [62/1438], Loss: 0.0651, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [62/1438], Loss: 0.0651, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [64/1438], Loss: 0.0936, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0246\n",
      "  loss_box_reg: 0.0120\n",
      "  loss_objectness: 0.0214\n",
      "  loss_rpn_box_reg: 0.0356\n",
      "Epoch [1], Batch [64/1438], Loss: 0.0936, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0246\n",
      "  loss_box_reg: 0.0120\n",
      "  loss_objectness: 0.0214\n",
      "  loss_rpn_box_reg: 0.0356\n",
      "Epoch [1], Batch [66/1438], Loss: 0.1805, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [66/1438], Loss: 0.1805, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [68/1438], Loss: 0.1275, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0265\n",
      "  loss_box_reg: 0.0135\n",
      "  loss_objectness: 0.0329\n",
      "  loss_rpn_box_reg: 0.0546\n",
      "Epoch [1], Batch [68/1438], Loss: 0.1275, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0265\n",
      "  loss_box_reg: 0.0135\n",
      "  loss_objectness: 0.0329\n",
      "  loss_rpn_box_reg: 0.0546\n",
      "Epoch [1], Batch [70/1438], Loss: 0.3083, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [70/1438], Loss: 0.3083, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [72/1438], Loss: 0.0884, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0308\n",
      "  loss_box_reg: 0.0162\n",
      "  loss_objectness: 0.0090\n",
      "  loss_rpn_box_reg: 0.0325\n",
      "Epoch [1], Batch [72/1438], Loss: 0.0884, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0308\n",
      "  loss_box_reg: 0.0162\n",
      "  loss_objectness: 0.0090\n",
      "  loss_rpn_box_reg: 0.0325\n",
      "Epoch [1], Batch [74/1438], Loss: 0.1190, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [74/1438], Loss: 0.1190, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [76/1438], Loss: 0.1781, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0816\n",
      "  loss_box_reg: 0.0351\n",
      "  loss_objectness: 0.0331\n",
      "  loss_rpn_box_reg: 0.0283\n",
      "Epoch [1], Batch [76/1438], Loss: 0.1781, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0816\n",
      "  loss_box_reg: 0.0351\n",
      "  loss_objectness: 0.0331\n",
      "  loss_rpn_box_reg: 0.0283\n",
      "Epoch [1], Batch [78/1438], Loss: 0.0790, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [78/1438], Loss: 0.0790, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [80/1438], Loss: 0.1060, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0428\n",
      "  loss_box_reg: 0.0284\n",
      "  loss_objectness: 0.0186\n",
      "  loss_rpn_box_reg: 0.0162\n",
      "Epoch [1], Batch [80/1438], Loss: 0.1060, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0428\n",
      "  loss_box_reg: 0.0284\n",
      "  loss_objectness: 0.0186\n",
      "  loss_rpn_box_reg: 0.0162\n",
      "Epoch [1], Batch [82/1438], Loss: 3.1558, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [82/1438], Loss: 3.1558, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [84/1438], Loss: 0.1463, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0612\n",
      "  loss_box_reg: 0.0426\n",
      "  loss_objectness: 0.0141\n",
      "  loss_rpn_box_reg: 0.0284\n",
      "Epoch [1], Batch [84/1438], Loss: 0.1463, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0612\n",
      "  loss_box_reg: 0.0426\n",
      "  loss_objectness: 0.0141\n",
      "  loss_rpn_box_reg: 0.0284\n",
      "Epoch [1], Batch [86/1438], Loss: 0.0653, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [86/1438], Loss: 0.0653, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [88/1438], Loss: 0.1598, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0246\n",
      "  loss_box_reg: 0.0156\n",
      "  loss_objectness: 0.0619\n",
      "  loss_rpn_box_reg: 0.0577\n",
      "Epoch [1], Batch [88/1438], Loss: 0.1598, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0246\n",
      "  loss_box_reg: 0.0156\n",
      "  loss_objectness: 0.0619\n",
      "  loss_rpn_box_reg: 0.0577\n",
      "Epoch [1], Batch [90/1438], Loss: 0.1884, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [90/1438], Loss: 0.1884, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [92/1438], Loss: 0.1471, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0306\n",
      "  loss_box_reg: 0.0136\n",
      "  loss_objectness: 0.0530\n",
      "  loss_rpn_box_reg: 0.0499\n",
      "Epoch [1], Batch [92/1438], Loss: 0.1471, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0306\n",
      "  loss_box_reg: 0.0136\n",
      "  loss_objectness: 0.0530\n",
      "  loss_rpn_box_reg: 0.0499\n",
      "Epoch [1], Batch [94/1438], Loss: 0.0903, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [94/1438], Loss: 0.0903, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [96/1438], Loss: 0.1198, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0390\n",
      "  loss_box_reg: 0.0183\n",
      "  loss_objectness: 0.0285\n",
      "  loss_rpn_box_reg: 0.0340\n",
      "Epoch [1], Batch [96/1438], Loss: 0.1198, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0390\n",
      "  loss_box_reg: 0.0183\n",
      "  loss_objectness: 0.0285\n",
      "  loss_rpn_box_reg: 0.0340\n",
      "Epoch [1], Batch [98/1438], Loss: 0.1073, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [98/1438], Loss: 0.1073, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [100/1438], Loss: 0.1414, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0503\n",
      "  loss_box_reg: 0.0185\n",
      "  loss_objectness: 0.0224\n",
      "  loss_rpn_box_reg: 0.0502\n",
      "Epoch [1], Batch [100/1438], Loss: 0.1414, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0503\n",
      "  loss_box_reg: 0.0185\n",
      "  loss_objectness: 0.0224\n",
      "  loss_rpn_box_reg: 0.0502\n",
      "Epoch [1], Batch [102/1438], Loss: 0.0881, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [102/1438], Loss: 0.0881, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [104/1438], Loss: 0.1334, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0565\n",
      "  loss_box_reg: 0.0326\n",
      "  loss_objectness: 0.0117\n",
      "  loss_rpn_box_reg: 0.0326\n",
      "Epoch [1], Batch [104/1438], Loss: 0.1334, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0565\n",
      "  loss_box_reg: 0.0326\n",
      "  loss_objectness: 0.0117\n",
      "  loss_rpn_box_reg: 0.0326\n",
      "Epoch [1], Batch [106/1438], Loss: 0.0898, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [106/1438], Loss: 0.0898, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [108/1438], Loss: 0.2181, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0529\n",
      "  loss_box_reg: 0.0120\n",
      "  loss_objectness: 0.1089\n",
      "  loss_rpn_box_reg: 0.0444\n",
      "Epoch [1], Batch [108/1438], Loss: 0.2181, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0529\n",
      "  loss_box_reg: 0.0120\n",
      "  loss_objectness: 0.1089\n",
      "  loss_rpn_box_reg: 0.0444\n",
      "Epoch [1], Batch [110/1438], Loss: 0.1158, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [110/1438], Loss: 0.1158, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [112/1438], Loss: 2.4598, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0543\n",
      "  loss_box_reg: 0.0202\n",
      "  loss_objectness: 0.5810\n",
      "  loss_rpn_box_reg: 1.8043\n",
      "Epoch [1], Batch [112/1438], Loss: 2.4598, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0543\n",
      "  loss_box_reg: 0.0202\n",
      "  loss_objectness: 0.5810\n",
      "  loss_rpn_box_reg: 1.8043\n",
      "Epoch [1], Batch [114/1438], Loss: 0.1680, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [114/1438], Loss: 0.1680, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [116/1438], Loss: 0.0922, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0322\n",
      "  loss_box_reg: 0.0176\n",
      "  loss_objectness: 0.0160\n",
      "  loss_rpn_box_reg: 0.0263\n",
      "Epoch [1], Batch [116/1438], Loss: 0.0922, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0322\n",
      "  loss_box_reg: 0.0176\n",
      "  loss_objectness: 0.0160\n",
      "  loss_rpn_box_reg: 0.0263\n",
      "Epoch [1], Batch [118/1438], Loss: 0.1468, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [118/1438], Loss: 0.1468, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [120/1438], Loss: 0.0652, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0156\n",
      "  loss_box_reg: 0.0044\n",
      "  loss_objectness: 0.0159\n",
      "  loss_rpn_box_reg: 0.0292\n",
      "Epoch [1], Batch [120/1438], Loss: 0.0652, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0156\n",
      "  loss_box_reg: 0.0044\n",
      "  loss_objectness: 0.0159\n",
      "  loss_rpn_box_reg: 0.0292\n",
      "Epoch [1], Batch [122/1438], Loss: 0.1258, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [122/1438], Loss: 0.1258, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [124/1438], Loss: 0.0869, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0265\n",
      "  loss_box_reg: 0.0142\n",
      "  loss_objectness: 0.0169\n",
      "  loss_rpn_box_reg: 0.0293\n",
      "Epoch [1], Batch [124/1438], Loss: 0.0869, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0265\n",
      "  loss_box_reg: 0.0142\n",
      "  loss_objectness: 0.0169\n",
      "  loss_rpn_box_reg: 0.0293\n",
      "Epoch [1], Batch [126/1438], Loss: 0.1052, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [126/1438], Loss: 0.1052, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [128/1438], Loss: 0.1274, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0449\n",
      "  loss_box_reg: 0.0307\n",
      "  loss_objectness: 0.0160\n",
      "  loss_rpn_box_reg: 0.0357\n",
      "Epoch [1], Batch [128/1438], Loss: 0.1274, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0449\n",
      "  loss_box_reg: 0.0307\n",
      "  loss_objectness: 0.0160\n",
      "  loss_rpn_box_reg: 0.0357\n",
      "Epoch [1], Batch [130/1438], Loss: 0.1196, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [130/1438], Loss: 0.1196, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [132/1438], Loss: 0.0532, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0187\n",
      "  loss_box_reg: 0.0072\n",
      "  loss_objectness: 0.0149\n",
      "  loss_rpn_box_reg: 0.0123\n",
      "Epoch [1], Batch [132/1438], Loss: 0.0532, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0187\n",
      "  loss_box_reg: 0.0072\n",
      "  loss_objectness: 0.0149\n",
      "  loss_rpn_box_reg: 0.0123\n",
      "Epoch [1], Batch [134/1438], Loss: 0.0931, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [134/1438], Loss: 0.0931, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [136/1438], Loss: 0.2633, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1247\n",
      "  loss_box_reg: 0.0772\n",
      "  loss_objectness: 0.0176\n",
      "  loss_rpn_box_reg: 0.0437\n",
      "Epoch [1], Batch [136/1438], Loss: 0.2633, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1247\n",
      "  loss_box_reg: 0.0772\n",
      "  loss_objectness: 0.0176\n",
      "  loss_rpn_box_reg: 0.0437\n",
      "Epoch [1], Batch [138/1438], Loss: 0.0572, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [138/1438], Loss: 0.0572, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [140/1438], Loss: 0.1075, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0360\n",
      "  loss_box_reg: 0.0234\n",
      "  loss_objectness: 0.0140\n",
      "  loss_rpn_box_reg: 0.0342\n",
      "Epoch [1], Batch [140/1438], Loss: 0.1075, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0360\n",
      "  loss_box_reg: 0.0234\n",
      "  loss_objectness: 0.0140\n",
      "  loss_rpn_box_reg: 0.0342\n",
      "Epoch [1], Batch [142/1438], Loss: 0.1315, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [142/1438], Loss: 0.1315, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [144/1438], Loss: 1.8942, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0650\n",
      "  loss_box_reg: 0.0308\n",
      "  loss_objectness: 0.5090\n",
      "  loss_rpn_box_reg: 1.2893\n",
      "Epoch [1], Batch [144/1438], Loss: 1.8942, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0650\n",
      "  loss_box_reg: 0.0308\n",
      "  loss_objectness: 0.5090\n",
      "  loss_rpn_box_reg: 1.2893\n",
      "Epoch [1], Batch [146/1438], Loss: 0.1274, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [146/1438], Loss: 0.1274, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [148/1438], Loss: 0.1032, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0393\n",
      "  loss_box_reg: 0.0241\n",
      "  loss_objectness: 0.0119\n",
      "  loss_rpn_box_reg: 0.0279\n",
      "Epoch [1], Batch [148/1438], Loss: 0.1032, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0393\n",
      "  loss_box_reg: 0.0241\n",
      "  loss_objectness: 0.0119\n",
      "  loss_rpn_box_reg: 0.0279\n",
      "Epoch [1], Batch [150/1438], Loss: 0.1368, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [150/1438], Loss: 0.1368, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [152/1438], Loss: 0.1039, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0327\n",
      "  loss_box_reg: 0.0139\n",
      "  loss_objectness: 0.0136\n",
      "  loss_rpn_box_reg: 0.0436\n",
      "Epoch [1], Batch [152/1438], Loss: 0.1039, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0327\n",
      "  loss_box_reg: 0.0139\n",
      "  loss_objectness: 0.0136\n",
      "  loss_rpn_box_reg: 0.0436\n",
      "Epoch [1], Batch [154/1438], Loss: 0.0670, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [154/1438], Loss: 0.0670, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [156/1438], Loss: 0.1557, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0549\n",
      "  loss_box_reg: 0.0230\n",
      "  loss_objectness: 0.0396\n",
      "  loss_rpn_box_reg: 0.0382\n",
      "Epoch [1], Batch [156/1438], Loss: 0.1557, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0549\n",
      "  loss_box_reg: 0.0230\n",
      "  loss_objectness: 0.0396\n",
      "  loss_rpn_box_reg: 0.0382\n",
      "Epoch [1], Batch [158/1438], Loss: 0.1381, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [158/1438], Loss: 0.1381, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [160/1438], Loss: 0.3100, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1465\n",
      "  loss_box_reg: 0.0762\n",
      "  loss_objectness: 0.0547\n",
      "  loss_rpn_box_reg: 0.0327\n",
      "Epoch [1], Batch [160/1438], Loss: 0.3100, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1465\n",
      "  loss_box_reg: 0.0762\n",
      "  loss_objectness: 0.0547\n",
      "  loss_rpn_box_reg: 0.0327\n",
      "Epoch [1], Batch [162/1438], Loss: 0.0636, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [162/1438], Loss: 0.0636, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [164/1438], Loss: 0.0804, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0337\n",
      "  loss_box_reg: 0.0193\n",
      "  loss_objectness: 0.0088\n",
      "  loss_rpn_box_reg: 0.0186\n",
      "Epoch [1], Batch [164/1438], Loss: 0.0804, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0337\n",
      "  loss_box_reg: 0.0193\n",
      "  loss_objectness: 0.0088\n",
      "  loss_rpn_box_reg: 0.0186\n",
      "Epoch [1], Batch [166/1438], Loss: 0.1441, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [166/1438], Loss: 0.1441, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [168/1438], Loss: 0.0714, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0170\n",
      "  loss_box_reg: 0.0045\n",
      "  loss_objectness: 0.0138\n",
      "  loss_rpn_box_reg: 0.0361\n",
      "Epoch [1], Batch [168/1438], Loss: 0.0714, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0170\n",
      "  loss_box_reg: 0.0045\n",
      "  loss_objectness: 0.0138\n",
      "  loss_rpn_box_reg: 0.0361\n",
      "Epoch [1], Batch [170/1438], Loss: 0.1556, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [170/1438], Loss: 0.1556, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [172/1438], Loss: 0.3398, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1603\n",
      "  loss_box_reg: 0.0745\n",
      "  loss_objectness: 0.0445\n",
      "  loss_rpn_box_reg: 0.0605\n",
      "Epoch [1], Batch [172/1438], Loss: 0.3398, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.1603\n",
      "  loss_box_reg: 0.0745\n",
      "  loss_objectness: 0.0445\n",
      "  loss_rpn_box_reg: 0.0605\n",
      "Epoch [1], Batch [174/1438], Loss: 0.1347, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [174/1438], Loss: 0.1347, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [176/1438], Loss: 0.1259, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0553\n",
      "  loss_box_reg: 0.0278\n",
      "  loss_objectness: 0.0217\n",
      "  loss_rpn_box_reg: 0.0212\n",
      "Epoch [1], Batch [176/1438], Loss: 0.1259, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0553\n",
      "  loss_box_reg: 0.0278\n",
      "  loss_objectness: 0.0217\n",
      "  loss_rpn_box_reg: 0.0212\n",
      "Epoch [1], Batch [178/1438], Loss: 0.2114, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [178/1438], Loss: 0.2114, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [180/1438], Loss: 0.0881, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0176\n",
      "  loss_box_reg: 0.0061\n",
      "  loss_objectness: 0.0128\n",
      "  loss_rpn_box_reg: 0.0516\n",
      "Epoch [1], Batch [180/1438], Loss: 0.0881, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0176\n",
      "  loss_box_reg: 0.0061\n",
      "  loss_objectness: 0.0128\n",
      "  loss_rpn_box_reg: 0.0516\n",
      "Epoch [1], Batch [182/1438], Loss: 0.2661, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [182/1438], Loss: 0.2661, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [184/1438], Loss: 0.1273, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0507\n",
      "  loss_box_reg: 0.0200\n",
      "  loss_objectness: 0.0218\n",
      "  loss_rpn_box_reg: 0.0348\n",
      "Epoch [1], Batch [184/1438], Loss: 0.1273, LR: 0.002000, GPU Memory: 0.37GB\n",
      "  loss_classifier: 0.0507\n",
      "  loss_box_reg: 0.0200\n",
      "  loss_objectness: 0.0218\n",
      "  loss_rpn_box_reg: 0.0348\n",
      "Epoch [1], Batch [186/1438], Loss: 0.1523, LR: 0.002000, GPU Memory: 0.37GB\n",
      "Epoch [1], Batch [186/1438], Loss: 0.1523, LR: 0.002000, GPU Memory: 0.37GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_faster_rcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed! You can now:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Check \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_faster_rcnn_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for the best model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 86\u001b[0m, in \u001b[0;36mtrain_faster_rcnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRINT_FREQ\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m     91\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m validate_model(model, valid_loader, config\u001b[38;5;241m.\u001b[39mDEVICE)\n",
      "Cell \u001b[0;32mIn[17], line 54\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 54\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS)  \u001b[38;5;66;03m# Store unscaled loss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Update weights every GRADIENT_ACCUMULATION_STEPS\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify data directory and yaml file exist\n",
    "    if not config.DATA_ROOT.exists():\n",
    "        print(f\"Error: Data directory not found at {config.DATA_ROOT}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    yaml_path = Path(config.DATA_YAML)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"Error: data.yaml not found at {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting Faster R-CNN training for AgroPest-12 dataset\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Classes: {config.NUM_CLASSES}\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"  Epochs: {config.NUM_EPOCHS}\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_faster_rcnn()\n",
    "    \n",
    "    print(\"\\nTraining completed! You can now:\")\n",
    "    print(\"1. Check 'best_faster_rcnn_model.pth' for the best model\")\n",
    "    print(\"2. View 'faster_rcnn_training_curves.png' for training progress\")\n",
    "    print(\"3. Run inference using the trained model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f82364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
